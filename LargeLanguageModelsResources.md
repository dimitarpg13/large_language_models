# Large Language Models Resources

## articles

* [Layer by Layer: Uncovering Hidden Representations in Language Models, Oscar Skein et al, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Layer_by_Layer-Uncovering_Hidden_Representations_in_Language_Models_Skean_2025.pdf)

* [Tracing the thoughts of a large language model, online blog, Anthropic, 2025](https://www.anthropic.com/research/tracing-thoughts-language-model)

* [Deep Seek Technical Report, DeepSeek AI, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/DeepSeek-V3_Technical_Report.pdf)

* [The DeepSeek Series: A Technical Overview, Shayan Mohanti at martinfowler.com, online article, 2025](https://martinfowler.com/articles/deepseek-papers.html)

* [Byte Latent Transformer: Patches Scale Better Than Tokens, Artidoro Pagnoni et al, FAIR at Meta, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/byte_latent_transformer/Byte_Latent_Transformer-Patches_Scale_Better_Than_Tokens_Pagnoni_2024.pdf)

  github repo: https://github.com/facebookresearch/blt

* [Addition is All You Need for Energy-efficient Language Models, H. Luo et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/optimizations_in_llm/Addition_is_All_You_Need_for_Energy-efficient_Language_Models_Luo_2024.pdf)

* [GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models, Iman Mirzadeh et al, Apple, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/human_like_reasoning/GSM-Symbolic-Understanding_the_Limitations_of_Mathematical_Reasoning_in_Large_Language_Models_Mizradeh_2024.pdf)

* [LLM Pruning and Distillation in Practice: The Minitron Approach, ST Sreenivas et al, NVidia, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/LLM_Pruning_and_Distillation_in_Practice-The_Minitron_Approach_Sriinivas_2024.pdf)

* [To Believe or Not to Believe Your LLM, Yasin Abbasi Yadkori et al, Google DeepMind, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/To_Believe_or_Not_to_Believe_Your_LLM_Yadkori_DeepMind_2024.pdf)

* [States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers, Ian Gemp et al, Google DeepMind, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/game_theory/States_as_Strings_as_Strategies-Steering_Language_Models_with_Game-Theoretic_Solvers_Gemp_DeepMind_2024.pdf)

* [The Consensus Game: Language Model Generation via Equilibrium Search, Athul Paul Jacob et al, MIT, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/game_theory/The_Consensus_Game-Language_Model_Generation_via_Equilibrium_Search_Jacob_MIT_2024.pdf)

* [Evaluating Reward Models for Language Modeling, N. Lambert et al, U. Washington, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Evaluating_Large_Language_Models_Trained_on_Code_OpenAI_2021.pdf)

* [Jamba: AHybrid Transformer-Mamba Language Model, Opher Lieber et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Jamba-A_Hybrid_Transformer-Mamba_Language_Model_Lieber_2024.pdf)

* [Better & Faster Large Language Models via Multi-token Prediction, Fabian Gloeckle et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Better_Faster_Large_Language_Models_via_Multi-token_Prediction_Gloeckle_2024.pdf)

* [The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text, Y. Guo et al, EP Paris, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/The_Curious_Decline_of_Linguistic_Diversity-Training_Language_Models_on_Synthetic_Text_Guo_EPL_2024.pdf)

* [OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework, Sachin Mehta et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/OpenELM-An_Efficient_Language_Model_Family_with_Open-source_Training_and_Inference_Framework_Mehta_2024.pdf)

* [FACtual enTailment fOr hallucInation Detection, Vipula Rawte et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/FACTOID-FACtual_enTailment_fOr_hallucInation_Detection_Rawte_2024.pdf)

* [Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks, Kim et al, Korea U., Imperial College, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Small_Language_Models_Learn_Enhanced_Reasoning_Skills_from_Medical_Textbooks_Kim_2024.pdf)

* [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention, T. Munkhdalai et al, Google, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Leave_No_Context_Behind-Efficient_Infinite_Context_Transformers_with_Infini-attention_Munkhadali_Google_2024.pdf)

* [Formal Aspects of Language Modeling, Ryan Cotterell, Anej Svete, Clara Meister, Tianyu Liu and Li Du, Lecture Notes, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Formal_Aspects_of_Language_Modeling_Ryan_Cotterell_2023.pdf)

* [ReALM:Reference Resolution As Language Modeling, Joel Ruben Antony Moniz et al, Apple, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/ReALM-Reference_Resolution_As_Language_Modeling_Moniz_Apple_2024.pdf)

* [A Neural Probabilistic Language Model, Y. Bengio et al, 2003](https://github.com/dimitarpg13/large_language_models/blob/main/articles/A_Neural_Probabilistic_Language_Model_bengio03a.pdf)

* [Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey, X. Liu et al, U of Maryland College Park, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Large_Language_Models_and_Causal_Inference_in_Collaboration-A_Comprehensive_Survey_Liu_UofMarylandCP_2024.pdf)

* [Unfamiliar Finetuning Examples Control How Language Models Hallucinate, Katie Kang et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Unfamiliar_Finetuning_Examples_Control_How_Language_Models_Hallucinate_Kang_2024.pdf)

* [Demystifying Embedding Spaces using Large Language Models, G. Tennenholtz et al, Google Research, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Demystifying_Embedding_Spaces_using_Large_Language_Models_Tennenholtz_2023.pdf)

* [MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training, B. McKinzie et al, Apple, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/MM1-Methods_Analysis_Insights_from_Multimodal_LLM_Pre-training_McKinzie_2024.pdf)

* [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking, Eric Zelikman et al, Stanford U., 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Quiet-STaR-Language_Models_Can_Teach_Themselves_to_Think_Before_Speaking_Zelikman_Stanford_2024.pdf)

* [Self-Discover: Large Language Models Self-Compose Reasoning Structures, P. Zhou et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Self-Discover-Large_Language_Models_Self-Compose_Reasoning_Structures_Zhou_2024.pdf)

* [DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models, Ollie Liu et al, USC, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/DeLLMa-A_Framework_for_Decision_Making_Under_Uncertainty_with_Large_Language_Models_Liu_2024.pdf)

* [Sora: A Review on Background, Technology, Limitations, and
Opportunities of Large Vision Models, Y. Liu et al, Lehigh U., Microsoft Research, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Sora-A_Review_on_Background_Technology_Limitations_and_Opportunities_of_Large_Vision_Models_Liu_2024.pdf)

* [Solving olympiad geometry without human demonstrations, TH Trinh et al, DeepMind, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Solving_olympiad_geometry_without_human_demonstrations_Trinh_DeepMind_2023.pdf)

* [Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training, E. Hubinger et al, Anthropic, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Sleeper_Agents-Training_Deceptive_LLMs_that_Persist_Through_Safety_Training_Hubinger_Anthropic_2024.pdf)

* [Self-Rewarding Language Models, W. Yuan et al, Meta, NYU, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Self-Rewarding_Language_Models_Yuan_2024.pdf)

* [Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts, M. Besta et al, ETH Zurich, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Topologies_of_Reasoning-Demystifying_Chains_Trees_and_Graphs_of_Thoughts_Besta_ETH_Zurich_2024.pdf)

* [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits, S. Ma et al , 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/The_Era_of_1-bit_LLMs-All_Large_Language_Models_are_in_1.58_Bits_Ma_2024.pdf)

* [Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding, X. Ning et al, Microsoft, Tsinghua U., 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Skeleton-of-Thought-Large_Language_Models_Can_Do_Parallel_Decoding_Ning_2023.pdf)

* [Chain of Code: Reasoning with a Language Model-Augmented Code Emulator, Li et al, DeepMind, Stanford U., UC Berkeley, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Chain_of_Code-Reasoning_with_a_Language_Model-Augmented_Code_Emulator_DeepMind_2023.pdf)

    related repo: https://sites.google.com/view/chain-of-code

* [Llama 2: Open Foundation and Fine-Tuned Chat Models, Hugo Touvron, Louis Martin, et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/LLaMAOpenandEfficientFoundationLanguageModels_MetaAI_2023.pdf)

* [GPT-4 Technical Report, OpenAI, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/GPT-4_Technical_Report_OpenAI_2023.pdf)

* [The Dawn of LMMs: Preliminary Explorations with GPT-4Vision, Yang et al, Microsoft, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/The_Dawn_of_LMMs-Preliminary_Explorations_with_GPT-4Vision_Yang_Microsoft_2023.pdf)

* [Sparks of artificial general intelligence: early experiments with GPT-4, Microsoft Research](https://github.com/dimitarpg13/large_language_models/blob/main/articles/SparksofArtificialGeneralIntelligenceEarlyExperimentswithGPT4.pdf)

* [Large Language Models can learn rules, Zhu et al, DeepMind, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Large_Language_Models_Can_Learn_Rules_Zhu_DeepMind_2023.pdf)

* [LLaMA: Open and Efficient Foundation Language Models, Meta AI](https://github.com/dimitarpg13/large_language_models/blob/main/articles/LLaMAOpenandEfficientFoundationLanguageModels_MetaAI_2023.pdf)

* [Physics of Large Language Models (Part 1), Context-free Grammar, Meta FAIR Labs, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Physics_of_Language_Models-Part%201_Context-Free_Grammar_Meta_FAIR_Labs_2023.pdf)

* [Evaluating Large Language Models is a minefield, A. Narayan, S. Kapoor, Princeton U., 2023, online blog](https://www.cs.princeton.edu/~arvindn/talks/evaluating_llms_minefield/)

* [Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks, M. Mitchell et al, Santa Fe Institute, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Comparing_Humans_GPT-4_and_GPT-4V_On_Abstraction_and_Reasoning_Tasks_Mitchell_SantaFe_2023.pdf)

* [Understanding LLMs: A Comprehensive Overview from Training to Inference, Liu et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Understanding_LLMs-A_Comprehensive_Overview_from_Training_to_Inference_Liu_2024.pdf)

* [Multimodality and Large Multimodal Models (LMMs), Chip Huyen, 2023, online article](https://huyenchip.com/2023/10/10/multimodal.html)

* [Branch-Solve-Merge Improves Large Language Model Evaluation and Generation, S. Saha et al, UNC Chapel Hill, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Branch-Solve-Merge_Improves_Large_Language_Model_Evaluation_and_Generation_Saha_UNCChapelHill_2023.pdf)

* [Introduction to Transformers: an NLP Perspective, T. Xiao et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Introduction_to_Transformers-a_NLP_Perspective_Xiao_2023.pdf)

* [Transformers Learn In-Context by Gradient Descent, Oswald et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Transformers_Learn_In-Context_by_Gradient_Descent_Oswald_2023.pdf)

* [Transformers as Algorithms: Generalization and Stability in In-context Learning, Li et al., 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Transformers_as_Algorithms-Generalization_and_Stability_in_In-context_Learning_Li_2023.pdf)

* [Hyena Hierarchy: Towards Larger Convolutional Language Models, Poli et al., 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Hyena_Hierarchy-Towards_Larger_Convolutional_Language_Models_Poli_Bengio_2023.pdf)

* [Toward Understanding Why Adam Converges faster Than SGD for Transformers, Pan et al., CMU, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Toward_Understanding_Why_Adam_Converges_Faster_Than_SGD_for_Transformers_CMU_2023.pdf)

* [Can GPT-3 Perform Statutory Reasoning?, Blair-Stanek, A et al., 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Can_GPT-3_Perform_Statutory_Reasoning_Blair-Stanek_2023.pdf)

* [An Explanation of In-context Learning as Implicity Bayesian Inference, Xie et al., Stanford, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/An_Explanation_of_In-context_Learning_as_Implicit_Bayesian_Inference_Stanford_2022.pdf)

* [Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions, S. Bhattamishra, Oxford U., 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Understanding_In-Context_Learning_in_Transformers_and_LLMs_by_Learning_to_Learn_Discrete_Functions_Bhattamishra_2023.pdf)

* [Efficient Transformers: A Survey, Tay et al., Google Research, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Efficient_Transformers-A_Survey_GoogleResearch_2022.pdf)

* [Emergent Abilities of Large Language Models, Wei et al., Google Research, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Emergent_Abilities_of_Large_Language_Models_GoogleResearch_2022.pdf)

* [A Path Towards Autonomous Machine Intelligence, Yann LeCun, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/A_Path_Towards_Autonomous_Machine_Intelligence_LeCunn_2022.pdf)

* [Holisitc Evaluation of Language Models, Center for Research on Foundation Models, Stanford, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Holistic_Evaluation_of_Language_Models_CRFM_StanfordU_2022.pdf)

* [A Systematic Evaluation of Large Language Models of Code, Xu et al., CMU, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/A_Systematic_Evaluation_of_Large_Language_Models_of_Code_CMU_2022.pdf)

* [Evaluating Large Language Models Trained on Code, Chen et al., OpenAI, 2021](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Evaluating_Large_Language_Models_Trained_on_Code_OpenAI_2021.pdf)

* [Language Models are Few-Shot Learners, Brown et al., OpenAI, 2020](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Language_Models_are_Few-ShotLearners_OpenAI2020.pdf)

* [Program Synthesis, Gulwani et al., Microsoft Research, 2017](https://github.com/dimitarpg13/large_language_models/blob/main/articles/program_synthesis_Gulwani_MicrosfotResearch_2017.pdf)

* [Adam: A Method for Stochasitc Optimization, D. Kingma et al, 2014](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Adam-A_method_for_stochastic_optimization_Kingma_2014.pdf)

* [Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning, Roemmele et al, 2011](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Choice_of_Plausible_Alternatives-An_Evaluation_of_Commonsense_Causal_Reasoning_Roemmele_2011.pdf)

* [Catastrophic Interference In Connectionist Networks: The Sequential Learning Problem, McCloskey, Cohen, 1989](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Catastrophic_Interference_in_Connectionist_Networks-The_Sequential_Learning_Problem_Mccloskey_Cohen_1989.pdf)

* [Attention Is All You Need, Vaswani et al, Google Brain, 2017](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Attention-is-all-you-need-NIPS-2017.pdf)

* [HyperAttention: Long-context Attention in Near-Linear Time, Insu Han et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/HyperAttention-Long-context_Attention_in_Near-Linear_Time_Han_2023.pdf)

* [The Annotated Transformer, 2018](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TheAnnotatedTransformer.pdf)

* [The Illustrated Transformer, Jay Alamar's blog, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TheIllustratedTransformer%E2%80%93JayAlammar%E2%80%93Visualizing_machine_learning_one_concept_at_a_time.pdf)

* [Attention in Natural Language Processing, Galassi et al., 2020](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/AttentionInNaturalLanguageProcessing.pdf)

* [Vision Language Transformers: A Survey, Clayton Fields, Casey Kennington, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Vision_Language_Transformers-A_Survey_Fields_2023.pdf)

* [Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization, Jin et al, Peking U., 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Unified_Language-Vision_Pretraining_in_LLM_with_Dynamic_Discrete_Visual_Tokenization_Jin_2023.pdf)

* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al., Google AI, 2019](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/BERTPretrainingofDeepBidirectionalTransformersforLanguageUnderstanding.pdf)

* [FAIRSEQ: A Fast, Extensible Toolkit for Sequence Modeling, Ott et al., 2019](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/FairseqAFastExtensibleToolkitForSequenceModeling.pdf)

* [Autoencoders, Dor Bank et al, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Autoencoders.pdf)

* [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling, Chung et al., 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/EmpiricalEvaluationOfGatedRecurrentNeuralNetworksonSequenceModeling.pdf)

* [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches, Cho et al., U de Montreal, 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/OnthePropertiesOfNeuralMachineTranslationEncoderDecoderApproaches.pdf)

* [Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network, A. Sherstinsky, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/FundamentalsOfRNNandLSTMNetwork.pdf)

* [A Decomposable Attention Model for Natural Language Inference, Parikh et al., Google Research, 2016](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/DecomposableAttentionModelforNaturalLanguageInferenceParikhUszkoreit2016.pdf)

* [Sequence to Sequence Learning with Neural Networks, Sutskever et al, Google Research, 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/SequencetoSequenceLearningwithNeuralNetworksSutsekver2014.pdf)

* [Transforming Auto-encoders, G. Hinton, A. Krizhevsky, et al., 2011](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TransformingAutoencodersHinton.pdf)

* [Long Short-Term Memory, Sepp Hochreiter et al., 1997](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/LongShortTermMemory.pdf)

* [Understanding LSTM: a tutorial into Long Short-Term Memory, R. Staudemeyer et al., 2019](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TutorialOnLongShortTermMemory2019.pdf)

* [What Can Transformers Learn in Context? A Case Study of Simple Function Classes, Carg S., et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/What_Can_Transformers_Learn%20In-Context-A_Case_Study_of_Simple_Function_Classes_Stanford_2023.pdf)

* [Toward Understanding Why Adam Converges Faster Than SGD for Transformers, Pan, Y, et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Toward_Understanding_Why_Adam_Converges_Faster_Than_SGD_for_Transformers_CMU_2023.pdf)

* [How ChatGPT Behavior is Changing Over Time?, Chen, L, Stanford, UC Berkeley, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/How_ChatGPT_behavior_is_changing_over_time_2023.pdf)

* [What Is ChatGPT Doing and Why Does It Work?, S. Wolfram, Feb 2023, online article](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)

* [Retentive Network: A Successor to Transformer for Large Language Models, Sun, Y., Microsoft Research, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Retentive_Network-A_Successor_to_Transformer_for_Large_Language_Models_2023.pdf)

* [Meta-Transformer: A Unifed Framework for Multi-Modal Learning, Zhang, Y., et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Meta-Transformer_Unified_Framework_for_Multi-Modal_Learning_2023.pdf)

* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin, Google AI, 2019](https://github.com/dimitarpg13/large_language_models/blob/main/articles/BERT-Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding_Devlin_Google_2019.pdf)

* [Improving Language Understanding by Generative Pre-Training, A. Redford, Open AI, 2018](https://github.com/dimitarpg13/large_language_models/tree/main/articles/Improving_Language_Understanding_by_Generative_Pre-Training_Redford_OpenAI_2018.pdf)

* [Llama 2: Open Foundation and Fine-Tuned Chat Models, H. Touvron, Meta, 2023](https://github.com/dimitarpg13/large_language_models/tree/main/articles/Llama2-Open_Foundation_and_Fine-Tuned_Chat_Models_Touvron_Meta_2023.pdf)

* [Scaling Language Models - Methods, Analysis and Insights from Training Gopher,JW Rae, DeepMind, 2021](https://github.com/dimitarpg13/large_language_models/tree/main/articles/Scaling_Language_Models-Methods_Analysis_Insights_from_Training_Gopher_Rae_DeepMind_2021.pdf)

* [Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering, Tal Ridnik et al, CodiumAI, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Code_Generation_with_AlphaCodium-From_Prompt_Engineering_to_Flow_Engineering_Ridnik_2024.pdf)

* [Pengi: An Audio Language Model for Audio Tasks, S. Deshmukh et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Pengi-An_Audio_Language_Model_for_Audio_Tasks_Deshmukh_2024.pdf)

* [A Comprehensive Overview of Large Language Models, H. Naveed et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/A_Comprehensive_Overview_of_Large_Language_Models_Naveed_2024.pdf)

* [Are Long-LLMs A Necessity For Long-Context Tasks?, H. Qian et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Are_Long-LLMs_A_Necessity_For_Long-Context_Tasks_Qian_2024.pdf)

* [Foundations of Large Language Models, T. Xiao et al, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Foundations_of_Large_Language_Models_Xiao_2025.pdf)

* [Frontier AI systems have surpassed the self-replicating red line, X. Pan et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/self_replication/Frontier_AI_systems_have_surpassed_the_self-replicating_red_line_Yang_2024.pdf)

* [A Primer on the Inner Workings of Transformer-Based Language Models, A Ferrando et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Primer_on_The_Inner_Workings_of_Transformer-Based_Language_Models_Ferrando_2024.pdf)

* [... More articles on Transformers](https://github.com/dimitarpg13/transformers_intro/tree/main/articles_and_books)

* [...More LLM articles on this repo](https://github.com/dimitarpg13/large_language_models/tree/main/articles)

### Fine Tuning LLMs

* [Overtrained Language Models Are Harder to Fine-Tune, Jacob Mitchell Springer et al, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/fine_tuning/Overtrained_Language_Models_Are_Harder_to_Fine-Tune_Springer_2025.pdf)

* [Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs, Jan Betley et al, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/fine_tuning/Emergent_Misalignment_Narrow_finetuning_can_produce_broadly_misaligned_LLMs_Betley_2025.pdf)

* [The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities, Venkatesh Balavadhani Parthasarathy et al, CEDAR Connect Group, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/fine_tuning/The_Ultimate_Guide_to_Fine_Tuning_LLMs_from_Basics_to_Breakthroughs_Balavadhani_2024.pdf)

* [Unfamiliar Finetuning Examples Control How Language Models Hallucinate, Katie Kang et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Unfamiliar_Finetuning_Examples_Control_How_Language_Models_Hallucinate_Kang_2024.pdf)

* [Vanishing Gradients in Reinforcement finetuning of Language Models, Noam Razin et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/reinforcement_learning/Vanishing_Gradients_in_Reinforcement_Finetuning_of_Language_Models_Razin_2024.pdf)

* [Fine-Tuning Large Language Models (LLMs) with Shawhin Talebi, Medium, 2023](https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91)

### Chain-of-Thought in LLMs

* [Adaptive Graph of Thoughts: Test-Time Adaptive Reasoning Unifying Chain, Tree, and Graph Structures, Tushar Pandey et al, Agnostiq, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/graph_models/Adaptive_Graph_of_Thoughts-Test-Time_Adaptive_Reasoning_Unifying_Chain_Tree_and_Graph_Structures_Pandey_2025.pdf)

* [Understanding Chain-of-Thought in LLMs through Information Theory, Jean-François Ton et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/CoT/Understanding_Chain-of-Thought_in_LLMs_through_Information_Theory_Ton_2024.pdf)

* [Self-Consistency Improves Chain-of-Thought Reasoning in Large Language Models, X. Wang et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Self-Consistency_Improves_Chain_of_Thought_Reasoning_in_Language_Models_Wang_2022.pdf)

### LLM-inspired techniques in statistical learning

* [Understanding LLM Embeddings for Regression, E. Tang et al, Stanford U., 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/embeddings/Understanding_LLM_Embeddings_for_Regression_Tang_2024.pdf)

### Human-like Reasoning and Representation Learning

* [Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models, Yukang Yang et al, Princeton U., 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/human_like_reasoning/Emergent_Symbolic_Mechanisms_Support_Abstract_Reasoning_in_Large_Language_Models_Yang_2025.pdf)

* [Concise Reasoning via Reinforcement Learning, M. Fatemi et al, Wand AI, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/human_like_reasoning/Concise_Reasoning_via_Reinforcement_Learning_Fatemi_2025.pdf)

* [Reasoning Models Don’t Always Say What They Think, Yanda Chen et al, Anthropic, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/human_like_reasoning/Reasoning_Models_Dont_Always_Say_What_They_Think_Chen_2025.pdf)

* [Reasoning to Learn from Latent Thoughts, Yangjun Ruan et al, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/human_like_reasoning/Reasoning_to_Learn_from_Latent_Thoughts_Ruan_2025.pdf)

* [LLM Post-Training: A Deep Dive into Reasoning Large Language Models, K. Kumar et al, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/human_like_reasoning/LLM_Post_Training_A_Deep_Dive_into_Reasoning_Large_Language_Models_Kumar_2025.pdf)

* [Evaluating the Systematic Reasoning Abilities of Large Language Models through Graph Coloring, Alex Heyman et al, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/human_like_reasoning/Evaluating_the_Systematic_Reasoning_Abilities_of_Large_Language_Models_through_Graph_Coloring_Hymann_2025.pdf)

* [Competitve Programming with Large Reasoning Models, OpenAI, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/human_like_reasoning/Competitive_Programming_with_Large_Reasoning_Models_El-Kishky_2025.pdf)

* [Reasoning Language Models: A Blueprint, M. Besta et al, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/human_like_reasoning/Reasoning_Language_Models-A_Blueprint_Besta_2025.pdf)

* [Reverse Thinking Makes LLMs Stronger Reasoners, J. Chen et al, UNC Chapel Hill. Google, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/human_like_reasoning/Reverse_Thinking_Makes_LLMs_Stronger_Reasoners_Chen_2024.pdf)

* [softmax is not enough (for sharp out-of-distribution), Peter Velickovic et al, DeepMind, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/human_like_reasoning/softmax_is_not_enough_for_sharp_out-of-distribution_Velickovic_2024.pdf)

* [Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models, ZR Tam et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/human_like_reasoning/Let_Me_Speak_Freely-A_Study_on_the_Impact_of_Format_Restrictions_on_Performance_of_Large_Language_Models_Tam_2024.pdf)

* [GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models, Iman Mirzadeh et al, Apple, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/human_like_reasoning/GSM-Symbolic-Understanding_the_Limitations_of_Mathematical_Reasoning_in_Large_Language_Models_Mizradeh_2024.pdf)

* [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking, Eric Zelikman et al, Stanford U., 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/human_like_reasoning/Quiet-STaR-Language_Models_Can_Teach_Themselves_to_Think_Before_Speaking_Zelikman_2024.pdf)

* [STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning, E. Zelikman et al, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/human_like_reasoning/STaR-Self-Taught_Reasoner_Bootstrapping_Reasoning_With_Reasoning_Zelikman_2022.pdf)

* [RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold, A. Setlur et al, CMU, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/human_like_reasoning/RL_on_Incorrect_Synthetic_Data_Scales_the_Efficiency_of_LLM_Math_Reasoning_by_Eight-Fold_Setlur_CMU_2024.pdf)

* [Improve Mathematical Reasoning in Language Models by Automated Process Supervision, L. Luo et al, DeepMind, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/human_like_reasoning/Improve_Mathematical_Reasoning_in_Language_Models_by_Automated_Process_Supervision_Luo_2024.pdf)

* [Solving math word problems with processand outcome-based feedback, J. Uesato et al, DeepMind, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/human_like_reasoning/Solving_math_word_problems_with_process_and_outcome-based_feedback_Uesato_DeepMind_2022.pdf)

### Agentic LLMs and Multi-Agent LLM Systems

* [Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems, B. Liu et al, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/agentic_LLMs_and_multi-agent_systems/Advances_and_Challenges_in_Foundation_Agents_Liu_2025.pdf)

* [AgentRxiv: Towards Collaborative Autonomous Research, Samuel Schmidgall et al, ETH Zurich, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/agentic_LLMs_and_multi-agent_systems/AgentRxiv-Towards_Collaborative_Autonomous_Research_Schmidtgall_ETHZurich_2025.pdf)

* [SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks, Y. Zhou et al, Meta FAIR, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/agentic_LLMs_and_multi-agent_systems/SWEET-RL-Training_Multi-Turn_LLM_Agents_on_Collaborative_Reasoning_Tasks_Zhou_2025.pdf)

* [LeanAgent: Lifelong Learning for Formal Theorem Proving, Adarsh Kumarappan et al, Caltech, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/agentic_LLMs_and_multi-agent_systems/LeanAgent-Lifelong_Learning_for_Formal_Theorem_Proving_Kumarkappan_2025.pdf)

* [Towards an AI co-scientist, Juraj Gottweis et al, Google DeepMind, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/agentic_LLMs_and_multi-agent_systems/Towards_an_AI_coscientist_Gottweiss_DeepMind_2025.pdf)

* [Mechanism Design for Large Language Models, Paul Duetting et al, February 13, 2025](https://research.google/blog/mechanism-design-for-large-language-models/)

* [Multi-Agent Systems: How Teams of LLMs Excell at Complex Tasks, May 2024](https://www.jonkrohn.com/posts/2024/5/31/multi-agent-systems-how-teams-of-llms-excel-at-complex-tasks)

* [Practical Considerations for Agentic LLM Systems, C. Sypherd et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/agentic_LLMs_and_multi-agent_systems/Practical_Considerations_for_Agentic_LLM_Systems_Sypherd_2024.pdf)

* [Building Agentic Systems in an Era of Large Language Models, Charles Packer, UC Berkeley, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/agentic_LLMs_and_multi-agent_systems/Building_Agentic_Systems_in_an_Era_of_Large_Language_Models_Packer_2024.pdf)

* [Understanding The Planning of LLM Agents: A Survey, X. Huang et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/agentic_LLMs_and_multi-agent_systems/Understanding_the_planning_of_LLM_agents-A_survey_Huang_2024.pdf)

* [Large Language Model Agent: A Survey on Methodology, Applications and Challenges, Junyu Luo et al, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/agentic_LLMs_and_multi-agent_systems/Large_Language_Model_Agent-A_Survey_on_Methodology_Applications_and_Challenges_Luo_2025.pdf)

### Large Concept Models

* [Large Concept Models: Language Modeling in a Sentence Representation Space, the LCM Team, Meta FAIR, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/large_concept_models/Large_Concept_Models-Language_Modeling_in_a_Sentence_Representation_Space_LCM_team_Meta_2024.pdf)

### Large Language Diffusion Models

* [Large Language Diffusion Models, S. Nie et al, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/large_lang_diffusion_models/Large_Language_Diffusion_Models_Nie_2025.pdf)

### Temporal Graph Neural Networks (TGNN)

* [Unifying Text Semantics and Graph Structures for Temporal Text-Attributed Graphs with Large Language Models, S. Zhang et al, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/temporal_GNN/Unifying_Text_Semantics_and_Graph_Structures_for_Temporal_Text-attributed_Graphs_with_Large_Language_Models_Zhang_2025.pdf)

### Reinforcement Learning Techniques in LLM

* [SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training, T. Chu, Y. Zhai, J. Yang, S. Tong,
S. Xie, Dale Schuurmans, QV. Le, Sergey Levine](https://github.com/dimitarpg13/large_language_models/blob/main/articles/reinforcement_learning/SFT_Memorizes_RL_Generalizes-A_Comparative_Study_of_Foundation_Model_Post-training_SergeyLevine_2025.pdf)

* [A Little Bit of Reinforcement Learning from Human Feedback: A short introduction to RLHF and post-training focused on
language models. Nathan Lambert, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/reinforcement_learning/A_Little_Bit_of_Reinforcement_Learning_from_Human_Feedback_Lambert_2025.pdf)

* [Robust Preference Optimization through Reward Model Distillation, Adam Fisch et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/reinforcement_learning/Robust_Preference_Optimization_through_Reward_Model_Distillation_Fisch_2024.pdf)

* [Training Language Models to Self-Correct via Reinforcement Learning, Aviral Kumar et al, Google DeepMind, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/reinforcement_learning/Training_Language_Models_to_Self-Correct_via_Reinforcement_Learning_Kumar_2024.pdf)

* [Asymptotics of Language Model Alignment, Joy Qiping Yang et al, U. of Sydney, Google DeepMind, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/reinforcement_learning/Asymptotics_of_Language_Model_Alignment_Yang_2024.pdf)

* [Controlled Decoding from Language Models, Sidharth Mudgal et al, 2024](https://github.com/dimitarpg13/reinforcement_learning_and_game_theory/blob/main/articles/ReinforcementLearning/reinforcement_learning_from_human_feedback/Controlled_Decoding_from_Language_Models_Mudgal_2024.pdf)

* [RL with KL penalties is better viewed as Bayesian inference, Tomasz Korbak et al, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/reinforcement_learning/RL_with_KL_penalties_is_better_viewed_as_Bayesian_inference_Korbak_2022.pdf)

* [Vanishing Gradients in Reinforcement finetuning of Language Models, Noam Razin et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/reinforcement_learning/Vanishing_Gradients_in_Reinforcement_Finetuning_of_Language_Models_Razin_2024.pdf)

* [Scaling Reinforcement Learning with LLMs, Technical Report, Kimi, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/reinforcement_learning/Scaling_RL_with_LLM_Kimi_k1.5.pdf)

* [DeepSeek R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, DeepSeek AI, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/reinforcement_learning/DeepSeek_R1-Incentivizing_Reasoning_Capability_in_LLMs_via_RL.pdf)

* [Deep Seek Prover v1.5 Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search, H. Xin et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/reinforcement_learning/DeepSeek-Prover-V1.5-Harnessing_Proof_Assistant_Feedback_for_Reinforcement_Learning_and_Monte-Carlo_Tree_Search.pdf)

* [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models, Z. Shao et al, DeepSeek AI, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/reinforcement_learning/DeepSeekMath-Pushing_the_Limits_of_Mathematical_Reasoning_in_Open_Language_Models_Shao_2024.pdf)

* [The DeepSeek Series: A Technical Overview, Shayan Mohanti at martinfowler.com, online article, 2025](https://martinfowler.com/articles/deepseek-papers.html)

* [Large Language Model Training and Reinforcement Learning, Miquel Noguer i Alonso, AIFI, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/reinforcement_learning/Large_Language_Model_Training_and_Reinforcement_Learning_Alonso_2025.pdf)

### Monte Carlo Tree Search in Large Language Models

* [Monte Carlo Tree Search: A Review of Recent Modifications and Applications, Maciej Świechowsk et al, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/MCTS/Monte_Carlo_Tree_Search-A_Review_of_Recent_Modifications_and_Applications_Swiechowski_2022.pdf)

* [Monte-Carlo tree search as regularized policy optimization, Jean-Bastien Grill et al, 2020](https://github.com/dimitarpg13/large_language_models/blob/main/articles/MCTS/Monte-Carlo_Tree_Search_as_Regularized_Policy_Optimization_Grill_2020.pdf)

* [Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search, Rémi Coulom, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/MCTS/Efficient_Selectivity_and_Backup_Operators_in_Monte-Carlo_Tree_Search_Remi_Coulom_2025.pdf)

### Neural Scaling

* [LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law, T. Liu et al, Cornell U, ICL, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/MC_learning/LLMs_learn_governing_principles_of_dynamical_systems_revealing_an_in-context_neural_scaling_law_Liu_2024.pdf)

* [Large Language Models as Markov Chains, O. Zekri et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/MC_learning/Large_Language_Models_as_Markov_Chains_Zekri_2024.pdf)

### Theorem Proving

* [LeanAgent: Lifelong Learning for Formal Theorem Proving, Adarsh Kumarappan et al, Caltech, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/theorem_proving/LeanAgent-Lifelong_Learning_for_Formal_Theorem_Proving_Kumarkappan_2025.pdf)

* [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models, Z. Shao et al, Tsinghua U., 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/theorem_proving/DeepSeekMath-Pushing_the_Limits_of_Mathematical_Reasoning_in_Open_Language_Models_Shao_TsinghuaU_2024.pdf)

* [Solving Olympiad Geometry without Human Demonstrations, TH Trinh et al, Google DeepMind, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/theorem_proving/Solving_olympiad_geometry_without_human_demonstrations_Trinh_Google_2023.pdf)

* [LeanDojo: Theorem Proving with Retrieval-Augmented Language Models, K. Yang et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/theorem_proving/LeanDojo-Theorem_Proving_with_Retrieval-Augmented_Language_Models_Yang_Caltech_2023.pdf)

* [NeurIPS Tutorial on Machine Learning for Theorem Proving, video](https://machine-learning-for-theorem-proving.github.io/)

* [DeepMath - Deep Sequence Models for Premise Selection, Alexander Alemi, Francois Chollet et al, 2016](https://github.com/dimitarpg13/large_language_models/blob/main/articles/theorem_proving/DeepMath-Deep_Sequence_Models_for_Premise_Selection_alemi_2016.pdf)

### LLM Tokenization

* [Let's build the GPT Tokenizer with Andrej Karpathy (February 2024)](https://youtu.be/zduSFxRajkE)
  
* [Minimal Byte Pair Encoding Algorithm (Andrej Karpathy repo)](https://github.com/karpathy/minbpe/tree/master)
  
* [Language Models are Unsupervised Multitask Learners, Alec Radford et al, 2018](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Language_Models_are_Unsupervised_Multitask_Learners_Alec_Radford_OpenAI_2018.pdf)
  
* [Neural Machine Translation of Rare Words with Subword Units, Rico Senrich et al, 2016](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Neural_Machine_Translation_of_Rare_Words_with_Subword_Units_Rico_Senrich_2016.pdf)

### Context Window representations and implementations

* [Towards infinite LLM context windows, Towards Data Science, Krzysztof K. Zdeb, 2024](https://towardsdatascience.com/towards-infinite-llm-context-windows-e099225abaaf)

* [RoFormer: Enhanced Transformer with Rotary Position Embedding, Jianlin Su et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/RoFormer-Enhanced_Transformer_with_Rotary_Position_Embedding_Su_2023.pdf)

* [YaRN: Efficient Context Window Extension of Large Language Models, B. Peng et al, U of Geneva, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/YaRN-Efficient_Context_Window_Extension_of_Large_Language_Models_Peng_UoGeneva_2023.pdf)

* [PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training, D. Zhu et al, Peking U., 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/PoSE-Efficient_Context_Window_Extension_of_LLMs_via_Positional_Skip-wise_Training_Zhu_Peking_U_2024.pdf)

* [LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens, Y. Ding et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/LongRoPE-Extending_LLM_Context_Window_Beyond_2_Million_Tokens_Ding_Microsoft_2024.pdf)

* [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention, T. Munkhdalai et al, Google, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Leave_No_Context_Behind-Efficient_Infinite_Context_Transformers_with_Infini-attention_Munkhadali_Google_2024.pdf)

### LLM Optimizer Design

* [Towards Efficient Optimizer Design for LLM via Structured Fisher Approximation with a Low-Rank Extension, W. Gong et al, Microsoft, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/LLM_optimizer_design/Towards_Efficient_Optimizer_Design_for_LLM_via_Structured_Fisher_Approximation_with_a_Low-Rank_Extension_Gong_Msft_2025.pdf)

### Time-series forecasting and classification tasks
* [iTransformer: The Latest Breakthrough in Time Series Forecasting, Marco Peixeiro, Towards Data Science, April 2024](https://towardsdatascience.com/itransformer-the-latest-breakthrough-in-time-series-forecasting-d538ddc6c5d1)

    relevant paper: [iTransformer: Inverted Transformers Are Effective for Time Series Forecasting, Yong Liu et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/time_series_forecasting/iTransformer-Inverted_Transformers_Are_Effective_for_Time_Series_Forecasting_Liu_2023.pdf)

* [MOMENT: A Foundation Model for Time Series Forecasting, Classification, Anomaly Detection, Nikos Kafritsas, Apr 27, 2024, Medium](https://towardsdatascience.com/moment-a-foundation-model-for-time-series-forecasting-classification-anomaly-detection-1e35f5b6ca76)

* [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, Colin Raffel et al, Google, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Exploring_the_Limits_of_Transfer_Learning_with_a_Unified_Text-to-Text_Transformer_Raffel_2023.pdf)

    relevant repo: https://github.com/google-research/text-to-text-transfer-transformer

* [TimesFM: Google's Foundation Model For Time-Series Forecasting, Nikos Kafritas, 2023, AI Horizon Forecast](https://aihorizonforecast.substack.com/p/timesfm-googles-foundation-model)

* [MOIRAI: Salesforce's Foundation Transformer For Time-Series Forecasting, Nikos Kafritas, 2023, AI Horizon Forecast](https://aihorizonforecast.substack.com/p/moirai-salesforces-foundation-transformer)

    relevant paper: [Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting, K. Rasul et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/time_series_forecasting/Lag-Llama-Towards_Foundation_Models_for_Probabilistic_Time_Series_Forecasting_Rasul_2023.pdf)

    relevant paper: [A decoder-only foundation model for time-series forecasting, A. Das et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/time_series_forecasting/A_decoder-only_foundation_model_for_time-series_forecasting_Das_2023.pdf)

    relevant paper: [Chronos: Learning the Language of Time Series, AF Ansari et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Chronos-Learning_the_Language_of_Time_Series_Fatir_2024.pdf)

    relevant paper: [Unified Training of Universal Time Series Forecasting Transformers, Woo, G et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Unified_Training_of_Universal_Time_Series_Forecasting_Transformers_Woo_2024.pdf)

* [How to Effectively Forecast Time Series with Amazon's New Time Series Forecasting Model, Eivind Kjosbakken, April 9, 2024, Towards Data Science](https://towardsdatascience.com/how-to-effectively-forecast-time-series-with-amazons-new-time-series-forecasting-model-9e04d4ccf67e)

    relevant paper: [Chronos: Learning the Language of Time Series, AF Ansari et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Chronos-Learning_the_Language_of_Time_Series_Fatir_2024.pdf)


* [TimeGPT: The First Foundation Model for Time Series Forecasting, Marco Peixeiro, October, 2023, Towards Data Science](https://towardsdatascience.com/timegpt-the-first-foundation-model-for-time-series-forecasting-bf0a75e63b3a)

* [TimeGPT-1, Azul Garza, Max Mergenthaler-Canseco, Nixtla, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/TimeGPT-1_Garza_Nixtla_2023.pdf)

* [Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series, Vijay Ekambaram et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Tiny_Time_Mixers-Fast_Pre-trained_Models_or_Enhanced_Zero_Few-Shot_Forecasting_of_Multivariate_Time_Series_Ekambaram_2024.pdf)

    TTM model and source code: https://huggingface.co/ibm/TTM, https://github.com/IBM/tsfm/tree/main/tsfm_public/models/tinytimemixer

* [Are Language Models Actually Useful for Time Series Forecasting? M. Tan et al, U. of Virginia, U. of Washington, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/time_series_forecasting/Are_Language_Models_Actually_Useful_for_Time_Series_Forecasting_Tan_2024.pdf)

### Retrieval-Augmented Generation (RAG)

* [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, Patrick Lewis et al, Facebook AI, UCL, 2021](https://github.com/dimitarpg13/large_language_models/blob/main/articles/rag/Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks_Lewis_2021.pdf)

* [Retrieval-Augmented Generation for Large Language Models: A Survey, Y. gao et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/rag/Retrieval-Augmented_Generation_for_Large_Language_Models-A_Survey_Gao_2024.pdf)

* [From Local to Global: A Graph RAG Approach to Query-Focused Summarization, Darren Edge et al, Microsoft Research, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/rag/From_Local_to_Global-A_Graph_RAG_Approach_to_Query-Focused_Summarization_Edge_Microsoft_2024.pdf)

    relevant repo: https://github.com/microsoft/graphrag

    relevant online article: https://www.microsoft.com/en-us/research/blog/graphrag-new-tool-for-complex-data-discovery-now-on-github/

* [Searching for Best Practices in Retrieval-Augmented Generation, X. Wang et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/rag/Searching_for_Best_Practices_in_Retrieval_Augmented_Generation_Wang_2024.pdf)

* [Graph Retrieval-Augmented Generation: A Survey, B. Peng et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/rag/Graph_Retrieval-Augmented_Generation-A_Survey_Peng_2024.pdf)

* [AirRAG: Activating Intrinsic Reasoning for Retrieval Augmented Generation via Tree-based Search, W. Feng et al, Alibaba, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/rag/AirRAG-Activating_Intrinsic_Reasoning_for_Retrieval_Augmented_Generation_via_Tree-based_Search_Wang_2025.pdf)

### Retrieval-Augmented Fine Tuning (RAFT)

* [RAFT:  A new way to teach LLMs to be better at RAG, Cedric Vidal, 2024](https://techcommunity.microsoft.com/t5/ai-ai-platform-blog/raft-a-new-way-to-teach-llms-to-be-better-at-rag/ba-p/4084674)

* [RAFT: Adapting Language Model to Domain Specific RAG, T. Zhang et al, 2024 (blog)](https://gorilla.cs.berkeley.edu/blogs/9_raft.html)

* [RAFT: Adapting Language Model to Domain Specific RAG, T. Zhang et al, 2024 (paper)](https://github.com/dimitarpg13/large_language_models/blob/main/articles/RAFT-Adapting_Language_Model_to_Domain_Specific_RAG_Zhang_2024.pdf)

relevant repos:
  * https://github.com/ShishirPatil/gorilla/tree/main/raft
  * https://github.com/ShishirPatil/gorilla

* [How to Build a Local Open-Source LLM Chatbot With RAG: Talking to PDF documents with Google’s Gemma-2b-it, LangChain, and Streamlit, Dr. Leon Eversberg, medium](https://towardsdatascience.com/how-to-build-a-local-open-source-llm-chatbot-with-rag-f01f73e2a131)

### The Attention Mechanicsm and Its Alternatives in Large Language Models

* [The FFTStrikes Back: An Efficient Alternative to Self-Attention, Jacob Fein-Ashley, U of SoCal, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/attention/The_FFT_Strikes_Back-An_Efficient_Alternative_to_Self-Attention_Fein-Ashley_2025.pdf)

* [Capturing Relative Positional Information Before RoPE In LLMs, Damien Benveniste, 2025](https://github.com/dimitarpg13/large_language_models/blob/main/articles/attention/Capturing_Relative_Positional_Information_Before_RoPE_In_LLMs_Benveniste_2025.pdf)

* [Contextual Position Encoding: Learning to Count What's Important, O. Golovneva et al, Meta, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/attention/Contextual_Position_Encoding-Learning_to_Count_Whats_Important_Golovneva_Meta_2024.pdf)

* [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention, T. Munkhdalai et al, Google, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Leave_No_Context_Behind-Efficient_Infinite_Context_Transformers_with_Infini-attention_Munkhadali_Google_2024.pdf)

* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness, T. Dao et al, Stanford U., 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/FlashAttention-Fast_and_Memory-Efficient_Exact_Attention_with_IO-Awareness_Stanford_2022.pdf)

* [HyperAttention: Long COntext Attention in Near Linear Time, Insu Han et al, Yale, Google Research, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/HyperAttention-Long-context_Attention_in_Near-Linear_Time_Han_2023.pdf)

* [Augmenting Language Models with Long Term Memory, W. Wang et al, UC Santa Barbara, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Augmenting_Language_Models_with_Long-Term_Memory_Wang_2023.pdf)

* [Attention in Natural Language Processing, Galassi et al., 2020](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/AttentionInNaturalLanguageProcessing.pdf)

* [Attention Is All You Need, Vaswani et al, Google Brain, 2017](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Attention-is-all-you-need-NIPS-2017.pdf)

* [HyperAttention: Long-context Attention in Near-Linear Time, Insu Han et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/HyperAttention-Long-context_Attention_in_Near-Linear_Time_Han_2023.pdf)

* [The Annotated Transformer, 2018](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TheAnnotatedTransformer.pdf)

* [The Illustrated Transformer, Jay Alamar's blog, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TheIllustratedTransformer%E2%80%93JayAlammar%E2%80%93Visualizing_machine_learning_one_concept_at_a_time.pdf)

* [Attention in Natural Language Processing, Galassi et al., 2020](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/AttentionInNaturalLanguageProcessing.pdf)

* [Eigen Analysis of Self-Attention and its Reconstruction from Partial Computation, Srinadh Bhojanapalli et al, Google Research, 2021](https://github.com/dimitarpg13/large_language_models/blob/main/articles/attention/Eigen_Analysis_of_Self-Attention_and_its_Reconstruction_from_Partial_Computation_Bhojanapali_2021.pdf)

* [A Decomposable Attention Model for Natural Language Inference, Parikh et al., Google Research, 2016](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/DecomposableAttentionModelforNaturalLanguageInferenceParikhUszkoreit2016.pdf)


### Compiler Optimization using LLM

* [Meta Large Language Model Compiler: Foundation Models of Compiler Optimization, C. Cummins et al, MetaAI, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/compiler_optimization/Meta_Large_Language_Model_Compiler-Foundation_Models_of_Compiler_Optimization_Cummins_MetaAI_2024.pdf)

  huggingface repo: [LLM compiler](https://huggingface.co/collections/facebook/llm-compiler-667c5b05557fe99a9edd25cb)

### Evaluation of LLMs

* [Evaluating LLM Systems: Essential Metrics, Benchmarks, and Best Practices, Jeffrey Ip, online article, 2024](https://www.confident-ai.com/blog/evaluating-llm-systems-metrics-benchmarks-and-best-practices)

* [LLM Evaluation Metrics: The Ultimate LLM Evaluation Guide, Jeffrey Ip, online article, 2024](https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation)

* `confident-ai` repo for LLM evaluation: https://github.com/confident-ai/deepeval

* [Unveiling LLM Evaluation Focused on Metrics: Challenges and Solutions, T. Hu et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/evaluation/Unveiling_LLM_Evaluation_Focused_on_Metrics-Challenges_and_Solutions_Hu_2024.pdf)

* [Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations, Evan Milller, Anthropic, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/evaluation/Adding_Error_Bars_to_Evals-A_Statistical_Approach_to_Language_Model_Evaluations_Miller_2024.pdf)

### DSPy
* [DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines, Omar Khattab et al, Stanford U., 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/DSPy/DSPy-Compiling_Declarative_Language_Model_Calls_into_Self-Improving_Pipelines_Khattab_2023.pdf)

   website: https://dspy.ai/

   repo: https://github.com/stanfordnlp/dspy

## LLM anti-hype reading list

* [The list by Vicki Boykis](https://gist.github.com/veekaybee/be375ab33085102f9027853128dc5f0e)

* What are empdeddings?

  [pdf document](https://github.com/veekaybee/what_are_embeddings/blob/main/embeddings.pdf)
  
  [code samples](https://github.com/veekaybee/what_are_embeddings/tree/main/notebooks)

* [Language Modeling is Compression, G. Deletang et al, ICRL 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Language_Modeling_Is_Compression_Deletang_ICLR2024.pdf)

* [Long Term Memory in LLM: notes on Vector Search and DB](https://github.com/edoliberty/vector-search-class-notes)

* [The Scaling Hypothesis](https://gwern.net/scaling-hypothesis)

* [Tokenization in NLP](https://github.com/SumanthRH/tokenization)

* [Chapter on Tokenization from HuggingFace NLP course](https://huggingface.co/learn/nlp-course/chapter6/1)

## online videos and blogs

* [GPT in 60 lines of NumPy code with Jay Mody, blog, (January, 2023)](https://jaykmody.com/blog/gpt-from-scratch/)
  
* [How ChatGPT is Trained with Ari Seff (February, 2023)](https://www.youtube.com/watch?v=VPRSBzXzavo)

* [Let's build GPT: from scratch, in code, spelled out with Andrej Karpathy (February 2023)](https://www.youtube.com/watch?v=kCc8FmEb1nY)

* [Let's build the GPT Tokenizer with Andrej Karpathy (February 2024)](https://youtu.be/zduSFxRajkE)

* [Stanford CS229 I Machine Learning I Building Large Language Models (LLMs)](https://youtu.be/9vM4p9NN0Ts?si=scotQ7vuGl0qcOtH)

* [Stanford CS336 Language Modeling from Scratch, Spring 2025](https://stanford-cs336.github.io/spring2025/)

## Resource on LLM visualization

The resource below attempts to visualize what is happening in LLM under the hood and is a helpful tool to comprehend the work of decoder-only Transformer-based LLMs. The author Brendan Bycroft has made an interesting attempt to visualize these structures and clarify how they operate. This webpage in the link below provides visualization for a family of GPT models, presented in 3D animations with walkthrough. The tool provides a step-by-step guide for single-token inference, coupled with interactive elements for a hands-on experience.

https://bbycroft.net/llm

## Articles on LLMs in Cornell University's Advancing AI for Humanity blog

The blog: https://thegenerality.com/agi/

some of the articles:

* [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits, S. Ma et al , 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/The_Era_of_1-bit_LLMs-All_Large_Language_Models_are_in_1.58_Bits_Ma_2024.pdf)

* [BitNet: Scaling 1-bit Transformers for Large Language Models](https://github.com/dimitarpg13/large_language_models/blob/main/articles/BitNet-Scaling_1-bit_Transformers_for_Large_Language_Models_Wang_2023.pdf)

* [Retentive Network: A Successor to Transformer for Large Language Models, Sun et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Retentive_Network-A_Successor_to_Transformer_for_Large_Language_Models_Sun_2023.pdf)

* [Large Language Model for Science: A Study on P vs. NP, Q. Dong et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Retentive_Network-A_Successor_to_Transformer_for_Large_Language_Models_Microsoft_Tsinghua_Sun_2023.pdf)

* [Augmenting Language Models with Long-Term Memory, W. Wang et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Augmenting_Language_Models_with_Long-Term_Memory_Wang_2023.pdf)

* [Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers, Dai et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Why_Can_GPT_Learn_In-Context_Language_Models_Implicitly_Perform_Gradient_Descent_as_Meta-Optimizers_Dai_2023.pdf)

* [LONGNET: Scaling Transformers to 1,000,000,000 Tokens, J. Ding et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/LongNet-Scaling_Transformers_to_one_billion_Tokens_Ding_2023.pdf)

* [A Length-Extrapolatable Transformer, Sun et al, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/A_Length-Extrapolatable_Transformer_Sun_2022.pdf)
  
## medium

* [The Transformer Architecture of GPT Models with Beatriz Stollniz](https://towardsdatascience.com/the-transformer-architecture-of-gpt-models-b8695b48728b)

* [Learning Transformers Code First Part 1 - The Setup with Lily Hughs-Robinson](https://towardsdatascience.com/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0)

* [Learning Transformers Code First Part 2 - GPT Up Close and Personal with Lily Hughs-Robinson](https://towardsdatascience.com/learning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7)

* [Understanding Large Language Models: The Physics of ChatGPT and BERT with Tim Lou](https://towardsdatascience.com/understanding-large-language-models-the-physics-of-chat-gpt-and-bert-ea512bcc6a64)

* [Transformer Architectures and the Rise of BERT, GPT, and T5: A Beginner's Guide with Manas Joshi](https://pub.towardsai.net/transformer-architectures-and-the-rise-of-bert-gpt-and-t5-a-beginners-guide-ea5e5bca923c)

* [Inside GPT - I: Understanding the text generation with Fatih Demirci](https://towardsdatascience.com/inside-gpt-i-1e8840ca8093)

* [Platypus: Quick, Cheap and Powerful LLM with Salvatore Raieli](https://levelup.gitconnected.com/platypus-quick-cheap-and-powerful-llm-404b86af8755)

* [Configuring Nemo-Guardrails Your Way: An Alternative Method for LLM with Masatake Hirono](https://towardsdatascience.com/configuring-nemo-guardrails-your-way-an-alternative-method-for-large-language-models-c82aaff78f6e)

* [ChatGPT stories compiled by Mateusz Wasalski](https://medium.com/@m.wasalski/list/chatgpt-3742c7a4727d)

* [RetNet: Transformer killer is here with Vishal Rajput](https://medium.com/aiguys/retnet-transformer-killer-is-here-1dc7f50d1205)

* [Fine-Tuning Large Language Models (LLMs) with Shawhin Talebi](https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91)

* [How to Build an LLM from Scratch with Shawhin Talebi](https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9)

* [Conversations as Directed Graphs with LangChain with Daniel Warfield](https://towardsdatascience.com/conversations-as-directed-graphs-with-lang-chain-46d70e1a846c)

* [Mastering Language Models with Samuel Montgomery](https://towardsdatascience.com/mastering-language-models-32e1d891511a)

* [Self-Supervised Learning Using Projection Heads with Daniel Warfield](https://towardsdatascience.com/self-supervised-learning-using-projection-heads-b77af3911d33)

* [Summing Coin Values in Images using Lang-SAM and Deep Learning with Gamze Zorlubas](https://towardsdatascience.com/coin-counting-using-lang-sam-b469827808a7)

* [‘Talk’ to Your SQL Database Using LangChain and Azure OpenAI with Satwiki De](https://towardsdatascience.com/talk-to-your-sql-database-using-langchain-and-azure-openai-bb79ad22c5e2)

* [RLHF: Reinforcement Learning from Human Feedback with Ms Aerin](https://automata88.medium.com/rlhf-reinforcement-learning-from-human-feedback-faa5ff4761d1)

    related paper: [Training language models to follow instructions
with human feedback, Ouyang et al, OpenAI, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Training_language_models_to_follow_instructions_with_human_feedback_Ouyang_2022.pdf)

    related code: [Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture](https://github.com/lucidrains/PaLM-rlhf-pytorch/tree/main)


* [How to Convert Any Text Into a Graph of Concepts with Rahul Nayak](https://towardsdatascience.com/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a)

    related repo: [Knowledge_Graph](https://github.com/rahulnyk/knowledge_graph)

* [LLMs for Everyone: Running LangChain and a MistralAI 7B Model in Google Colab with Dmitrii Eliuseev](https://towardsdatascience.com/llms-for-everyone-running-langchain-and-a-mistralai-7b-model-in-google-colab-246ca94d7c4d)

* [LLMs for Everyone: Running the LLaMA-13B model and LangChain in Google Colab with Dmitrii Eliuseev](https://towardsdatascience.com/llms-for-everyone-running-the-llama-13b-model-and-langchain-in-google-colab-68d88021cf0b)

    related repo: https://github.com/ggerganov/llama.cpp

    related repo: https://github.com/langchain-ai/langchain

    related repo: https://colab.research.google.com/

* [Is Mamba the End of ChatGPT As We Know It? Igancio de Gregorio](https://pub.towardsai.net/is-mamba-the-end-of-chatgpt-as-we-know-it-a2ce57de0b02)

    related paper: [Mamba: Linear-Time Sequence Modeling with Selective State Spaces, A. Gu et al, CMU, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Mamba-Linear-Time_Sequence_Modeling_with_Selective_State_Spaces_Gu_CMU_2024.pdf)

* [RLAIF: Reinforcement Learning from AI Feedback with Cameron R. Wolfe, Jan, 2024](https://towardsdatascience.com/rlaif-reinforcement-learning-from-ai-feedback-d7dbdae8f093)

    related paper: [RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback, Harrison Lee et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/RLAIF-Scaling_Reinforcement_Learning_from_Human_Feedback_with_AI_Feedback_Lee_2023.pdf)

    related paper: [Constitutional AI: Harmlessness from AI Feedback, Y. Bai, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Constitutional_AI-Harmlessness_from_AI_Feedback_Bai_2022.pdf)

    related paper: [PaLM: Scaling Language Modeling with Pathways, A. Chowdhery et al, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/PaLM-Scaling_Language_Modeling_with_Pathways_Chowdhery_2022.pdf)

    related paper: [PaLM 2 Technical Report, Google, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/PaLM_2_Technical_Report_Anil_Google_2023.pdf)

    related paper: [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, Wei et al, Google Research, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Chain-of-Thought_Prompting_Elicits_Reasoning_in_Large_Language_Models_Wei_2023.pdf)

    related paper: [Self-Consistency Improves Chain of Thought Reasoning in Language Models, Wang et al, Google Research, ICLR 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Self-Consistency_Improves_Chain_of_Thought_Reasoning_in_Language_Models_Wang_2022.pdf)

    related paper: [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, Bai et al, Anthropic, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Training_a_Helpful_and_Harmless_Assistant_with_Reinforcement_Learning_from_Human_Feedback_Bai_2022.pdf)

    related paper: [A General Language Assistant as a Laboratory for Alignment, A. Askell et al, Anthropic, 2021](https://github.com/dimitarpg13/large_language_models/blob/main/articles/A_General_Language_Assistant_as_a_Laboratory_for_Alignment_Askell_Anthropic_2021.pdf)

    related paper: [Learning to summarize from human feedback, N. Stiennon et al, OpenAI, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Learning_to_summarize_from_human_feedback_Stiennon_OpenAI_2022.pdf)

* [Mistral AI vs. Meta: Comparing Top Open-source LLMs with Luis Roque, Jan 2024](https://towardsdatascience.com/mistral-ai-vs-meta-comparing-top-open-source-llms-565c1bc1516e)

* [Text Embeddings, Classification, and Semantic Search, Shaw Talebi, March 2024](https://towardsdatascience.com/text-embeddings-classification-and-semantic-search-8291746220be)

* [How to Build a Local Open-Source LLM Chatbot With RAG, Dr. Leon Eversberg, April, 2024](https://towardsdatascience.com/how-to-build-a-local-open-source-llm-chatbot-with-rag-f01f73e2a131)

* [What is a 1-bit LLM? — Bitnet.cpp may eliminate GPUs, Don Lim, 2024](https://medium.com/@don-lim/what-is-1-bit-llm-bitnet-cpp-may-eliminate-gpus-54b6e0d7207b)

    repo: https://github.com/microsoft/BitNet

    related paper: [BitNet: Scaling 1-bit Transformers for Large Language Models, H. Wang et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/quantization/BitNet-Scaling_1-bit_Transformers_for_Large_Language_Models_Wang_2023.pdf)

    related paper: [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits, S. Ma et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/quantization/The_Era_of_1-bit_LLMs-All_Large_Language_Models_are_in_1.58_Bits_Ma_2024.pdf)

    related online article: [Honey, I shrunk the LLM! A beginner's guide to quantization – and testing it, Tobias Mann, 2024](https://www.theregister.com/2024/07/14/quantization_llm_feature/)

    related online article: [1-bit LLMs Could Solve AI’s Energy Demands “Imprecise” language models are smaller, speedier—and nearly as accurate, Matthew Hutson, IEEE Spectrum, 2024](https://spectrum.ieee.org/1-bit-llm)
