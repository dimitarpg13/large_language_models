# Large Language Models Resources

## articles
* [To Believe or Not to Believe Your LLM, Yasin Abbasi Yadkori et al, Google DeepMind, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/To_Believe_or_Not_to_Believe_Your_LLM_Yadkori_DeepMind_2024.pdf)

* [States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers, Ian Gemp et al, Google DeepMind, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/game_theory/States_as_Strings_as_Strategies-Steering_Language_Models_with_Game-Theoretic_Solvers_Gemp_DeepMind_2024.pdf)

* [The Consensus Game: Language Model Generation via Equilibrium Search, Athul Paul Jacob et al, MIT, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/game_theory/The_Consensus_Game-Language_Model_Generation_via_Equilibrium_Search_Jacob_MIT_2024.pdf)

* [Evaluating Reward Models for Language Modeling, N. Lambert et al, U. Washington, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Evaluating_Large_Language_Models_Trained_on_Code_OpenAI_2021.pdf)

* [Jamba: AHybrid Transformer-Mamba Language Model, Opher Lieber et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Jamba-A_Hybrid_Transformer-Mamba_Language_Model_Lieber_2024.pdf)

* [Better & Faster Large Language Models via Multi-token Prediction, Fabian Gloeckle et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Better_Faster_Large_Language_Models_via_Multi-token_Prediction_Gloeckle_2024.pdf)

* [The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text, Y. Guo et al, EP Paris, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/The_Curious_Decline_of_Linguistic_Diversity-Training_Language_Models_on_Synthetic_Text_Guo_EPL_2024.pdf)

* [OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework, Sachin Mehta et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/OpenELM-An_Efficient_Language_Model_Family_with_Open-source_Training_and_Inference_Framework_Mehta_2024.pdf)

* [FACtual enTailment fOr hallucInation Detection, Vipula Rawte et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/FACTOID-FACtual_enTailment_fOr_hallucInation_Detection_Rawte_2024.pdf)

* [Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks, Kim et al, Korea U., Imperial College, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Small_Language_Models_Learn_Enhanced_Reasoning_Skills_from_Medical_Textbooks_Kim_2024.pdf)

* [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention, T. Munkhdalai et al, Google, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Leave_No_Context_Behind-Efficient_Infinite_Context_Transformers_with_Infini-attention_Munkhadali_Google_2024.pdf)

* [Formal Aspects of Language Modeling, Ryan Cotterell, Anej Svete, Clara Meister, Tianyu Liu and Li Du, Lecture Notes, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Formal_Aspects_of_Language_Modeling_Ryan_Cotterell_2023.pdf)

* [ReALM:Reference Resolution As Language Modeling, Joel Ruben Antony Moniz et al, Apple, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/ReALM-Reference_Resolution_As_Language_Modeling_Moniz_Apple_2024.pdf)

* [A Neural Probabilistic Language Model, Y. Bengio et al, 2003](https://github.com/dimitarpg13/large_language_models/blob/main/articles/A_Neural_Probabilistic_Language_Model_bengio03a.pdf)

* [Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey, X. Liu et al, U of Maryland College Park, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Large_Language_Models_and_Causal_Inference_in_Collaboration-A_Comprehensive_Survey_Liu_UofMarylandCP_2024.pdf)

* [Unfamiliar Finetuning Examples Control How Language Models Hallucinate, Katie Kang et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Unfamiliar_Finetuning_Examples_Control_How_Language_Models_Hallucinate_Kang_2024.pdf)

* [Demystifying Embedding Spaces using Large Language Models, G. Tennenholtz et al, Google Research, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Demystifying_Embedding_Spaces_using_Large_Language_Models_Tennenholtz_2023.pdf)

* [MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training, B. McKinzie et al, Apple, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/MM1-Methods_Analysis_Insights_from_Multimodal_LLM_Pre-training_McKinzie_2024.pdf)

* [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking, Eric Zelikman et al, Stanford U., 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Quiet-STaR-Language_Models_Can_Teach_Themselves_to_Think_Before_Speaking_Zelikman_Stanford_2024.pdf)

* [Self-Discover: Large Language Models Self-Compose Reasoning Structures, P. Zhou et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Self-Discover-Large_Language_Models_Self-Compose_Reasoning_Structures_Zhou_2024.pdf)

* [DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models, Ollie Liu et al, USC, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/DeLLMa-A_Framework_for_Decision_Making_Under_Uncertainty_with_Large_Language_Models_Liu_2024.pdf)

* [Sora: A Review on Background, Technology, Limitations, and
Opportunities of Large Vision Models, Y. Liu et al, Lehigh U., Microsoft Research, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Sora-A_Review_on_Background_Technology_Limitations_and_Opportunities_of_Large_Vision_Models_Liu_2024.pdf)

* [Solving olympiad geometry without human demonstrations, TH Trinh et al, DeepMind, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Solving_olympiad_geometry_without_human_demonstrations_Trinh_DeepMind_2023.pdf)

* [Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training, E. Hubinger et al, Anthropic, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Sleeper_Agents-Training_Deceptive_LLMs_that_Persist_Through_Safety_Training_Hubinger_Anthropic_2024.pdf)

* [Self-Rewarding Language Models, W. Yuan et al, Meta, NYU, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Self-Rewarding_Language_Models_Yuan_2024.pdf)

* [Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts, M. Besta et al, ETH Zurich, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Topologies_of_Reasoning-Demystifying_Chains_Trees_and_Graphs_of_Thoughts_Besta_ETH_Zurich_2024.pdf)

* [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits, S. Ma et al , 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/The_Era_of_1-bit_LLMs-All_Large_Language_Models_are_in_1.58_Bits_Ma_2024.pdf)

* [Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding, X. Ning et al, Microsoft, Tsinghua U., 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Skeleton-of-Thought-Large_Language_Models_Can_Do_Parallel_Decoding_Ning_2023.pdf)

* [Chain of Code: Reasoning with a Language Model-Augmented Code Emulator, Li et al, DeepMind, Stanford U., UC Berkeley, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Chain_of_Code-Reasoning_with_a_Language_Model-Augmented_Code_Emulator_DeepMind_2023.pdf)

    related repo: https://sites.google.com/view/chain-of-code

* [Llama 2: Open Foundation and Fine-Tuned Chat Models, Hugo Touvron, Louis Martin, et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/LLaMAOpenandEfficientFoundationLanguageModels_MetaAI_2023.pdf)

* [GPT-4 Technical Report, OpenAI, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/GPT-4_Technical_Report_OpenAI_2023.pdf)

* [The Dawn of LMMs: Preliminary Explorations with GPT-4Vision, Yang et al, Microsoft, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/The_Dawn_of_LMMs-Preliminary_Explorations_with_GPT-4Vision_Yang_Microsoft_2023.pdf)

* [Sparks of artificial general intelligence: early experiments with GPT-4, Microsoft Research](https://github.com/dimitarpg13/large_language_models/blob/main/articles/SparksofArtificialGeneralIntelligenceEarlyExperimentswithGPT4.pdf)

* [Large Language Models can learn rules, Zhu et al, DeepMind, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Large_Language_Models_Can_Learn_Rules_Zhu_DeepMind_2023.pdf)

* [LLaMA: Open and Efficient Foundation Language Models, Meta AI](https://github.com/dimitarpg13/large_language_models/blob/main/articles/LLaMAOpenandEfficientFoundationLanguageModels_MetaAI_2023.pdf)

* [Physics of Large Language Models (Part 1), Context-free Grammar, Meta FAIR Labs, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Physics_of_Language_Models-Part%201_Context-Free_Grammar_Meta_FAIR_Labs_2023.pdf)

* [Evaluating Large Language Models is a minefield, A. Narayan, S. Kapoor, Princeton U., 2023, online blog](https://www.cs.princeton.edu/~arvindn/talks/evaluating_llms_minefield/)

* [Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks, M. Mitchell et al, Santa Fe Institute, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Comparing_Humans_GPT-4_and_GPT-4V_On_Abstraction_and_Reasoning_Tasks_Mitchell_SantaFe_2023.pdf)

* [Understanding LLMs: A Comprehensive Overview from Training to Inference, Liu et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Understanding_LLMs-A_Comprehensive_Overview_from_Training_to_Inference_Liu_2024.pdf)

* [Multimodality and Large Multimodal Models (LMMs), Chip Huyen, 2023, online article](https://huyenchip.com/2023/10/10/multimodal.html)

* [Branch-Solve-Merge Improves Large Language Model Evaluation and Generation, S. Saha et al, UNC Chapel Hill, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Branch-Solve-Merge_Improves_Large_Language_Model_Evaluation_and_Generation_Saha_UNCChapelHill_2023.pdf)

* [Introduction to Transformers: an NLP Perspective, T. Xiao et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Introduction_to_Transformers-a_NLP_Perspective_Xiao_2023.pdf)

* [Transformers Learn In-Context by Gradient Descent, Oswald et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Transformers_Learn_In-Context_by_Gradient_Descent_Oswald_2023.pdf)

* [Transformers as Algorithms: Generalization and Stability in In-context Learning, Li et al., 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Transformers_as_Algorithms-Generalization_and_Stability_in_In-context_Learning_Li_2023.pdf)

* [Hyena Hierarchy: Towards Larger Convolutional Language Models, Poli et al., 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Hyena_Hierarchy-Towards_Larger_Convolutional_Language_Models_Poli_Bengio_2023.pdf)

* [Toward Understanding Why Adam Converges faster Than SGD for Transformers, Pan et al., CMU, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Toward_Understanding_Why_Adam_Converges_Faster_Than_SGD_for_Transformers_CMU_2023.pdf)

* [Can GPT-3 Perform Statutory Reasoning?, Blair-Stanek, A et al., 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Can_GPT-3_Perform_Statutory_Reasoning_Blair-Stanek_2023.pdf)

* [An Explanation of In-context Learning as Implicity Bayesian Inference, Xie et al., Stanford, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/An_Explanation_of_In-context_Learning_as_Implicit_Bayesian_Inference_Stanford_2022.pdf)

* [Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions, S. Bhattamishra, Oxford U., 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Understanding_In-Context_Learning_in_Transformers_and_LLMs_by_Learning_to_Learn_Discrete_Functions_Bhattamishra_2023.pdf)

* [Efficient Transformers: A Survey, Tay et al., Google Research, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Efficient_Transformers-A_Survey_GoogleResearch_2022.pdf)

* [Emergent Abilities of Large Language Models, Wei et al., Google Research, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Emergent_Abilities_of_Large_Language_Models_GoogleResearch_2022.pdf)

* [A Path Towards Autonomous Machine Intelligence, Yann LeCun, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/A_Path_Towards_Autonomous_Machine_Intelligence_LeCunn_2022.pdf)

* [Holisitc Evaluation of Language Models, Center for Research on Foundation Models, Stanford, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Holistic_Evaluation_of_Language_Models_CRFM_StanfordU_2022.pdf)

* [A Systematic Evaluation of Large Language Models of Code, Xu et al., CMU, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/A_Systematic_Evaluation_of_Large_Language_Models_of_Code_CMU_2022.pdf)

* [Evaluating Large Language Models Trained on Code, Chen et al., OpenAI, 2021](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Evaluating_Large_Language_Models_Trained_on_Code_OpenAI_2021.pdf)

* [Language Models are Few-Shot Learners, Brown et al., OpenAI, 2020](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Language_Models_are_Few-ShotLearners_OpenAI2020.pdf)

* [Program Synthesis, Gulwani et al., Microsoft Research, 2017](https://github.com/dimitarpg13/large_language_models/blob/main/articles/program_synthesis_Gulwani_MicrosfotResearch_2017.pdf)

* [Adam: A Method for Stochasitc Optimization, D. Kingma et al, 2014](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Adam-A_method_for_stochastic_optimization_Kingma_2014.pdf)

* [Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning, Roemmele et al, 2011](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Choice_of_Plausible_Alternatives-An_Evaluation_of_Commonsense_Causal_Reasoning_Roemmele_2011.pdf)

* [Catastrophic Interference In Connectionist Networks: The Sequential Learning Problem, McCloskey, Cohen, 1989](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Catastrophic_Interference_in_Connectionist_Networks-The_Sequential_Learning_Problem_Mccloskey_Cohen_1989.pdf)

* [Attention Is All You Need, Vaswani et al, Google Brain, 2017](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Attention-is-all-you-need-NIPS-2017.pdf)

* [HyperAttention: Long-context Attention in Near-Linear Time, Insu Han et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/HyperAttention-Long-context_Attention_in_Near-Linear_Time_Han_2023.pdf)

* [The Annotated Transformer, 2018](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TheAnnotatedTransformer.pdf)

* [The Illustrated Transformer, Jay Alamar's blog, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TheIllustratedTransformer%E2%80%93JayAlammar%E2%80%93Visualizing_machine_learning_one_concept_at_a_time.pdf)

* [Attention in Natural Language Processing, Galassi et al., 2020](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/AttentionInNaturalLanguageProcessing.pdf)

* [Vision Language Transformers: A Survey, Clayton Fields, Casey Kennington, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Vision_Language_Transformers-A_Survey_Fields_2023.pdf)

* [Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization, Jin et al, Peking U., 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Unified_Language-Vision_Pretraining_in_LLM_with_Dynamic_Discrete_Visual_Tokenization_Jin_2023.pdf)

* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al., Google AI, 2019](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/BERTPretrainingofDeepBidirectionalTransformersforLanguageUnderstanding.pdf)

* [FAIRSEQ: A Fast, Extensible Toolkit for Sequence Modeling, Ott et al., 2019](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/FairseqAFastExtensibleToolkitForSequenceModeling.pdf)

* [Autoencoders, Dor Bank et al, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Autoencoders.pdf)

* [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling, Chung et al., 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/EmpiricalEvaluationOfGatedRecurrentNeuralNetworksonSequenceModeling.pdf)

* [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches, Cho et al., U de Montreal, 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/OnthePropertiesOfNeuralMachineTranslationEncoderDecoderApproaches.pdf)

* [Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network, A. Sherstinsky, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/FundamentalsOfRNNandLSTMNetwork.pdf)

* [A Decomposable Attention Model for Natural Language Inference, Parikh et al., Google Research, 2016](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/DecomposableAttentionModelforNaturalLanguageInferenceParikhUszkoreit2016.pdf)

* [Sequence to Sequence Learning with Neural Networks, Sutskever et al, Google Research, 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/SequencetoSequenceLearningwithNeuralNetworksSutsekver2014.pdf)

* [Transforming Auto-encoders, G. Hinton, A. Krizhevsky, et al., 2011](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TransformingAutoencodersHinton.pdf)

* [Long Short-Term Memory, Sepp Hochreiter et al., 1997](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/LongShortTermMemory.pdf)

* [Understanding LSTM: a tutorial into Long Short-Term Memory, R. Staudemeyer et al., 2019](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TutorialOnLongShortTermMemory2019.pdf)

* [What Can Transformers Learn in Context? A Case Study of Simple Function Classes, Carg S., et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/What_Can_Transformers_Learn%20In-Context-A_Case_Study_of_Simple_Function_Classes_Stanford_2023.pdf)

* [Toward Understanding Why Adam Converges Faster Than SGD for Transformers, Pan, Y, et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Toward_Understanding_Why_Adam_Converges_Faster_Than_SGD_for_Transformers_CMU_2023.pdf)

* [How ChatGPT Behavior is Changing Over Time?, Chen, L, Stanford, UC Berkeley, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/How_ChatGPT_behavior_is_changing_over_time_2023.pdf)

* [What Is ChatGPT Doing and Why Does It Work?, S. Wolfram, Feb 2023, online article](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)

* [Retentive Network: A Successor to Transformer for Large Language Models, Sun, Y., Microsoft Research, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Retentive_Network-A_Successor_to_Transformer_for_Large_Language_Models_2023.pdf)

* [Meta-Transformer: A Unifed Framework for Multi-Modal Learning, Zhang, Y., et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Meta-Transformer_Unified_Framework_for_Multi-Modal_Learning_2023.pdf)

* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin, Google AI, 2019](https://github.com/dimitarpg13/large_language_models/blob/main/articles/BERT-Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding_Devlin_Google_2019.pdf)

* [Improving Language Understanding by Generative Pre-Training, A. Redford, Open AI, 2018](https://github.com/dimitarpg13/large_language_models/tree/main/articles/Improving_Language_Understanding_by_Generative_Pre-Training_Redford_OpenAI_2018.pdf)

* [Llama 2: Open Foundation and Fine-Tuned Chat Models, H. Touvron, Meta, 2023](https://github.com/dimitarpg13/large_language_models/tree/main/articles/Llama2-Open_Foundation_and_Fine-Tuned_Chat_Models_Touvron_Meta_2023.pdf)

* [Scaling Language Models - Methods, Analysis and Insights from Training Gopher,JW Rae, DeepMind, 2021](https://github.com/dimitarpg13/large_language_models/tree/main/articles/Scaling_Language_Models-Methods_Analysis_Insights_from_Training_Gopher_Rae_DeepMind_2021.pdf)

* [Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering, Tal Ridnik et al, CodiumAI, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Code_Generation_with_AlphaCodium-From_Prompt_Engineering_to_Flow_Engineering_Ridnik_2024.pdf)

* [Pengi: An Audio Language Model for Audio Tasks, S. Deshmukh et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Pengi-An_Audio_Language_Model_for_Audio_Tasks_Deshmukh_2024.pdf)

* [A Comprehensive Overview of Large Language Models, H. Naveed et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/A_Comprehensive_Overview_of_Large_Language_Models_Naveed_2024.pdf)

* [Are Long-LLMs A Necessity For Long-Context Tasks?, H. Qian et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Are_Long-LLMs_A_Necessity_For_Long-Context_Tasks_Qian_2024.pdf)

* [... More articles on Transformers](https://github.com/dimitarpg13/transformers_intro/tree/main/articles_and_books)

* [...More LLM articles on this repo](https://github.com/dimitarpg13/large_language_models/tree/main/articles)

### Human-like Reasoning and Representation Learning

* [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking, Eric Zelikman et al, Stanford U., 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Quiet-STaR-Language_Models_Can_Teach_Themselves_to_Think_Before_Speaking_Zelikman_Stanford_2024.pdf)

* [STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning, E. Zelikman et al, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/STaR-Self-Taught_Reasoner_Bootstrapping_Reasoning_With_Reasoning_Zelikman_2022.pdf)

### Theorem Proving

* [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models, Z. Shao et al, Tsinghua U., 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/theorem_proving/DeepSeekMath-Pushing_the_Limits_of_Mathematical_Reasoning_in_Open_Language_Models_Shao_TsinghuaU_2024.pdf)

* [Solving Olympiad Geometry without Human Demonstrations, TH Trinh et al, Google DeepMind, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/theorem_proving/Solving_olympiad_geometry_without_human_demonstrations_Trinh_Google_2023.pdf)

* [LeanDojo: Theorem Proving with Retrieval-Augmented Language Models, K. Yang et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/theorem_proving/LeanDojo-Theorem_Proving_with_Retrieval-Augmented_Language_Models_Yang_Caltech_2023.pdf)

* [NeurIPS Tutorial on Machine Learning for Theorem Proving, video](https://machine-learning-for-theorem-proving.github.io/)

* [DeepMath - Deep Sequence Models for Premise Selection, Alexander Alemi, Francois Chollet et al, 2016](https://github.com/dimitarpg13/large_language_models/blob/main/articles/theorem_proving/DeepMath-Deep_Sequence_Models_for_Premise_Selection_alemi_2016.pdf)

### LLM Tokenization

* [Let's build the GPT Tokenizer with Andrej Karpathy (February 2024)](https://youtu.be/zduSFxRajkE)
  
* [Minimal Byte Pair Encoding Algorithm (Andrej Karpathy repo)](https://github.com/karpathy/minbpe/tree/master)
  
* [Language Models are Unsupervised Multitask Learners, Alec Radford et al, 2018](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Language_Models_are_Unsupervised_Multitask_Learners_Alec_Radford_OpenAI_2018.pdf)
  
* [Neural Machine Translation of Rare Words with Subword Units, Rico Senrich et al, 2016](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Neural_Machine_Translation_of_Rare_Words_with_Subword_Units_Rico_Senrich_2016.pdf)

### Context Window representations and implementations

* [Towards infinite LLM context windows, Towards Data Science, Krzysztof K. Zdeb, 2024](https://towardsdatascience.com/towards-infinite-llm-context-windows-e099225abaaf)

* [RoFormer: Enhanced Transformer with Rotary Position Embedding, Jianlin Su et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/RoFormer-Enhanced_Transformer_with_Rotary_Position_Embedding_Su_2023.pdf)

* [YaRN: Efficient Context Window Extension of Large Language Models, B. Peng et al, U of Geneva, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/YaRN-Efficient_Context_Window_Extension_of_Large_Language_Models_Peng_UoGeneva_2023.pdf)

* [PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training, D. Zhu et al, Peking U., 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/PoSE-Efficient_Context_Window_Extension_of_LLMs_via_Positional_Skip-wise_Training_Zhu_Peking_U_2024.pdf)

* [LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens, Y. Ding et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/LongRoPE-Extending_LLM_Context_Window_Beyond_2_Million_Tokens_Ding_Microsoft_2024.pdf)

* [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention, T. Munkhdalai et al, Google, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Leave_No_Context_Behind-Efficient_Infinite_Context_Transformers_with_Infini-attention_Munkhadali_Google_2024.pdf)

### Time-series forecasting and classification tasks
* [iTransformer: The Latest Breakthrough in Time Series Forecasting, Marco Peixeiro, Towards Data Science, April 2024](https://towardsdatascience.com/itransformer-the-latest-breakthrough-in-time-series-forecasting-d538ddc6c5d1)

    relevant paper: [iTransformer: Inverted Transformers Are Effective for Time Series Forecasting, Yong Liu et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/time_series_forecasting/iTransformer-Inverted_Transformers_Are_Effective_for_Time_Series_Forecasting_Liu_2023.pdf)

* [MOMENT: A Foundation Model for Time Series Forecasting, Classification, Anomaly Detection, Nikos Kafritsas, Apr 27, 2024, Medium](https://towardsdatascience.com/moment-a-foundation-model-for-time-series-forecasting-classification-anomaly-detection-1e35f5b6ca76)

* [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, Colin Raffel et al, Google, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Exploring_the_Limits_of_Transfer_Learning_with_a_Unified_Text-to-Text_Transformer_Raffel_2023.pdf)

    relevant repo: https://github.com/google-research/text-to-text-transfer-transformer

* [TimesFM: Google's Foundation Model For Time-Series Forecasting, Nikos Kafritas, 2023, AI Horizon Forecast](https://aihorizonforecast.substack.com/p/timesfm-googles-foundation-model)

* [MOIRAI: Salesforce's Foundation Transformer For Time-Series Forecasting, Nikos Kafritas, 2023, AI Horizon Forecast](https://aihorizonforecast.substack.com/p/moirai-salesforces-foundation-transformer)

    relevant paper: [Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting, K. Rasul et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Lag-Llama-Towards_Foundation_Models_for_Probabilistic_Time_Series_Forecasting_Rasul_2023.pdf)

    relevant paper: [A decoder-only foundation model for time-series forecasting, A. Das et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/A_decoder-only_foundation_model_for_time-series_forecasting_Das_2023.pdf)

    relevant paper: [Chronos: Learning the Language of Time Series, AF Ansari et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Chronos-Learning_the_Language_of_Time_Series_Fatir_2024.pdf)

    relevant paper: [Unified Training of Universal Time Series Forecasting Transformers, Woo, G et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Unified_Training_of_Universal_Time_Series_Forecasting_Transformers_Woo_2024.pdf)

* [How to Effectively Forecast Time Series with Amazon's New Time Series Forecasting Model, Eivind Kjosbakken, April 9, 2024, Towards Data Science](https://towardsdatascience.com/how-to-effectively-forecast-time-series-with-amazons-new-time-series-forecasting-model-9e04d4ccf67e)

    relevant paper: [Chronos: Learning the Language of Time Series, AF Ansari et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Chronos-Learning_the_Language_of_Time_Series_Fatir_2024.pdf)


* [TimeGPT: The First Foundation Model for Time Series Forecasting, Marco Peixeiro, October, 2023, Towards Data Science](https://towardsdatascience.com/timegpt-the-first-foundation-model-for-time-series-forecasting-bf0a75e63b3a)

* [TimeGPT-1, Azul Garza, Max Mergenthaler-Canseco, Nixtla, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/TimeGPT-1_Garza_Nixtla_2023.pdf)

* [Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series, Vijay Ekambaram et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Tiny_Time_Mixers-Fast_Pre-trained_Models_or_Enhanced_Zero_Few-Shot_Forecasting_of_Multivariate_Time_Series_Ekambaram_2024.pdf)

    TTM model and source code: https://huggingface.co/ibm/TTM, https://github.com/IBM/tsfm/tree/main/tsfm_public/models/tinytimemixer

* [Are Language Models Actually Useful for Time Series Forecasting? M. Tan et al, U. of Virginia, U. of Washington, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/time_series_forecasting/Are_Language_Models_Actually_Useful_for_Time_Series_Forecasting_Tan_2024.pdf)

### Retrieval-Augmented Generation (RAG)

* [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, Patrick Lewis et al, Facebook AI, UCL, 2021](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Retrieval-Augmented_Generation_for_Knowledge-Intensive_NLP_Tasks_Lewis_2021.pdf)

* [Retrieval-Augmented Generation for Large Language Models: A Survey, Y. gao et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Retrieval-Augmented_Generation_for_Large_Language_Models-A_Survey_Gao_2024.pdf)

### Retrieval-Augmented Fine Tuning (RAFT)

* [RAFT:  A new way to teach LLMs to be better at RAG, Cedric Vidal, 2024](https://techcommunity.microsoft.com/t5/ai-ai-platform-blog/raft-a-new-way-to-teach-llms-to-be-better-at-rag/ba-p/4084674)

* [RAFT: Adapting Language Model to Domain Specific RAG, T. Zhang et al, 2024 (blog)](https://gorilla.cs.berkeley.edu/blogs/9_raft.html)

* [RAFT: Adapting Language Model to Domain Specific RAG, T. Zhang et al, 2024 (paper)](https://github.com/dimitarpg13/large_language_models/blob/main/articles/RAFT-Adapting_Language_Model_to_Domain_Specific_RAG_Zhang_2024.pdf)

relevant repos:
  * https://github.com/ShishirPatil/gorilla/tree/main/raft
  * https://github.com/ShishirPatil/gorilla

* [How to Build a Local Open-Source LLM Chatbot With RAG: Talking to PDF documents with Google’s Gemma-2b-it, LangChain, and Streamlit, Dr. Leon Eversberg, medium](https://towardsdatascience.com/how-to-build-a-local-open-source-llm-chatbot-with-rag-f01f73e2a131)

### The Attention Mechanicsm in Large Language Models

* [Contextual Position Encoding: Learning to Count What's Important, O. Golovneva et al, Meta, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/attention/Contextual_Position_Encoding-Learning_to_Count_Whats_Important_Golovneva_Meta_2024.pdf)

* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness, T. Dao et al, Stanford U., 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/FlashAttention-Fast_and_Memory-Efficient_Exact_Attention_with_IO-Awareness_Stanford_2022.pdf)

* [HyperAttention: Long COntext Attention in Near Linear Time, Insu Han et al, Yale, Google Research, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/HyperAttention-Long-context_Attention_in_Near-Linear_Time_Han_2023.pdf)

* [Augmenting Language Models with Long Term Memory, W. Wang et al, UC Santa Barbara, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Augmenting_Language_Models_with_Long-Term_Memory_Wang_2023.pdf)

### Compiler Optimization using LLM

* [Meta Large Language Model Compiler: Foundation Models of Compiler Optimization, C. Cummins et al, MetaAI, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/compiler_optimization/Meta_Large_Language_Model_Compiler-Foundation_Models_of_Compiler_Optimization_Cummins_MetaAI_2024.pdf)

  huggingface repo: [LLM compiler](https://huggingface.co/collections/facebook/llm-compiler-667c5b05557fe99a9edd25cb)

## online videos and blogs

* [GPT in 60 lines of NumPy code with Jay Mody, blog, (January, 2023)](https://jaykmody.com/blog/gpt-from-scratch/)
  
* [How ChatGPT is Trained with Ari Seff (February, 2023)](https://www.youtube.com/watch?v=VPRSBzXzavo)

* [Let's build GPT: from scratch, in code, spelled out with Andrej Karpathy (February 2023)](https://www.youtube.com/watch?v=kCc8FmEb1nY)

* [Let's build the GPT Tokenizer with Andrej Karpathy (February 2024)](https://youtu.be/zduSFxRajkE)

## Resource on LLM visualization

The resource below attempts to visualize what is happening in LLM under the hood and is a helpful tool to comprehend the work of decoder-only Transformer-based LLMs. The author Brendan Bycroft has made an interesting attempt to visualize these structures and clarify how they operate. This webpage in the link below provides visualization for a family of GPT models, presented in 3D animations with walkthrough. The tool provides a step-by-step guide for single-token inference, coupled with interactive elements for a hands-on experience.

https://bbycroft.net/llm

## Articles on LLMs in Cornell University's Advancing AI for Humanity blog

The blog: https://thegenerality.com/agi/

some of the articles:

* [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits, S. Ma et al , 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/The_Era_of_1-bit_LLMs-All_Large_Language_Models_are_in_1.58_Bits_Ma_2024.pdf)

* [BitNet: Scaling 1-bit Transformers for Large Language Models](https://github.com/dimitarpg13/large_language_models/blob/main/articles/BitNet-Scaling_1-bit_Transformers_for_Large_Language_Models_Wang_2023.pdf)

* [Retentive Network: A Successor to Transformer for Large Language Models, Sun et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Retentive_Network-A_Successor_to_Transformer_for_Large_Language_Models_Sun_2023.pdf)

* [Large Language Model for Science: A Study on P vs. NP, Q. Dong et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Retentive_Network-A_Successor_to_Transformer_for_Large_Language_Models_Microsoft_Tsinghua_Sun_2023.pdf)

* [Augmenting Language Models with Long-Term Memory, W. Wang et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Augmenting_Language_Models_with_Long-Term_Memory_Wang_2023.pdf)

* [Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers, Dai et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Why_Can_GPT_Learn_In-Context_Language_Models_Implicitly_Perform_Gradient_Descent_as_Meta-Optimizers_Dai_2023.pdf)

* [LONGNET: Scaling Transformers to 1,000,000,000 Tokens, J. Ding et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/LongNet-Scaling_Transformers_to_one_billion_Tokens_Ding_2023.pdf)

* [A Length-Extrapolatable Transformer, Sun et al, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/A_Length-Extrapolatable_Transformer_Sun_2022.pdf)
  
## medium

* [The Transformer Architecture of GPT Models with Beatriz Stollniz](https://towardsdatascience.com/the-transformer-architecture-of-gpt-models-b8695b48728b)

* [Learning Transformers Code First Part 1 - The Setup with Lily Hughs-Robinson](https://towardsdatascience.com/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0)

* [Learning Transformers Code First Part 2 - GPT Up Close and Personal with Lily Hughs-Robinson](https://towardsdatascience.com/learning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7)

* [Understanding Large Language Models: The Physics of ChatGPT and BERT with Tim Lou](https://towardsdatascience.com/understanding-large-language-models-the-physics-of-chat-gpt-and-bert-ea512bcc6a64)

* [Transformer Architectures and the Rise of BERT, GPT, and T5: A Beginner's Guide with Manas Joshi](https://pub.towardsai.net/transformer-architectures-and-the-rise-of-bert-gpt-and-t5-a-beginners-guide-ea5e5bca923c)

* [Inside GPT - I: Understanding the text generation with Fatih Demirci](https://towardsdatascience.com/inside-gpt-i-1e8840ca8093)

* [Platypus: Quick, Cheap and Powerful LLM with Salvatore Raieli](https://levelup.gitconnected.com/platypus-quick-cheap-and-powerful-llm-404b86af8755)

* [Configuring Nemo-Guardrails Your Way: An Alternative Method for LLM with Masatake Hirono](https://towardsdatascience.com/configuring-nemo-guardrails-your-way-an-alternative-method-for-large-language-models-c82aaff78f6e)

* [ChatGPT stories compiled by Mateusz Wasalski](https://medium.com/@m.wasalski/list/chatgpt-3742c7a4727d)

* [RetNet: Transformer killer is here with Vishal Rajput](https://medium.com/aiguys/retnet-transformer-killer-is-here-1dc7f50d1205)

* [Fine-Tuning Large Language Models (LLMs) with Shawhin Talebi](https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91)

* [How to Build an LLM from Scratch with Shawhin Talebi](https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9)

* [Conversations as Directed Graphs with LangChain with Daniel Warfield](https://towardsdatascience.com/conversations-as-directed-graphs-with-lang-chain-46d70e1a846c)

* [Mastering Language Models with Samuel Montgomery](https://towardsdatascience.com/mastering-language-models-32e1d891511a)

* [Self-Supervised Learning Using Projection Heads with Daniel Warfield](https://towardsdatascience.com/self-supervised-learning-using-projection-heads-b77af3911d33)

* [Summing Coin Values in Images using Lang-SAM and Deep Learning with Gamze Zorlubas](https://towardsdatascience.com/coin-counting-using-lang-sam-b469827808a7)

* [‘Talk’ to Your SQL Database Using LangChain and Azure OpenAI with Satwiki De](https://towardsdatascience.com/talk-to-your-sql-database-using-langchain-and-azure-openai-bb79ad22c5e2)

* [RLHF: Reinforcement Learning from Human Feedback with Ms Aerin](https://automata88.medium.com/rlhf-reinforcement-learning-from-human-feedback-faa5ff4761d1)

    related paper: [Training language models to follow instructions
with human feedback, Ouyang et al, OpenAI, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Training_language_models_to_follow_instructions_with_human_feedback_Ouyang_2022.pdf)

    related code: [Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture](https://github.com/lucidrains/PaLM-rlhf-pytorch/tree/main)


* [How to Convert Any Text Into a Graph of Concepts with Rahul Nayak](https://towardsdatascience.com/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a)

    related repo: [Knowledge_Graph](https://github.com/rahulnyk/knowledge_graph)

* [LLMs for Everyone: Running LangChain and a MistralAI 7B Model in Google Colab with Dmitrii Eliuseev](https://towardsdatascience.com/llms-for-everyone-running-langchain-and-a-mistralai-7b-model-in-google-colab-246ca94d7c4d)

* [LLMs for Everyone: Running the LLaMA-13B model and LangChain in Google Colab with Dmitrii Eliuseev](https://towardsdatascience.com/llms-for-everyone-running-the-llama-13b-model-and-langchain-in-google-colab-68d88021cf0b)

    related repo: https://github.com/ggerganov/llama.cpp

    related repo: https://github.com/langchain-ai/langchain

    related repo: https://colab.research.google.com/

* [Is Mamba the End of ChatGPT As We Know It? Igancio de Gregorio](https://pub.towardsai.net/is-mamba-the-end-of-chatgpt-as-we-know-it-a2ce57de0b02)

    related paper: [Mamba: Linear-Time Sequence Modeling with Selective State Spaces, A. Gu et al, CMU, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Mamba-Linear-Time_Sequence_Modeling_with_Selective_State_Spaces_Gu_CMU_2024.pdf)

* [RLAIF: Reinforcement Learning from AI Feedback with Cameron R. Wolfe, Jan, 2024](https://towardsdatascience.com/rlaif-reinforcement-learning-from-ai-feedback-d7dbdae8f093)

    related paper: [RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback, Harrison Lee et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/RLAIF-Scaling_Reinforcement_Learning_from_Human_Feedback_with_AI_Feedback_Lee_2023.pdf)

    related paper: [Constitutional AI: Harmlessness from AI Feedback, Y. Bai, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Constitutional_AI-Harmlessness_from_AI_Feedback_Bai_2022.pdf)

    related paper: [PaLM: Scaling Language Modeling with Pathways, A. Chowdhery et al, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/PaLM-Scaling_Language_Modeling_with_Pathways_Chowdhery_2022.pdf)

    related paper: [PaLM 2 Technical Report, Google, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/PaLM_2_Technical_Report_Anil_Google_2023.pdf)

    related paper: [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, Wei et al, Google Research, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Chain-of-Thought_Prompting_Elicits_Reasoning_in_Large_Language_Models_Wei_2023.pdf)

    related paper: [Self-Consistency Improves Chain of Thought Reasoning in Language Models, Wang et al, Google Research, ICLR 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Self-Consistency_Improves_Chain_of_Thought_Reasoning_in_Language_Models_Wang_2022.pdf)

    related paper: [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, Bai et al, Anthropic, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Training_a_Helpful_and_Harmless_Assistant_with_Reinforcement_Learning_from_Human_Feedback_Bai_2022.pdf)

    related paper: [A General Language Assistant as a Laboratory for Alignment, A. Askell et al, Anthropic, 2021](https://github.com/dimitarpg13/large_language_models/blob/main/articles/A_General_Language_Assistant_as_a_Laboratory_for_Alignment_Askell_Anthropic_2021.pdf)

    related paper: [Learning to summarize from human feedback, N. Stiennon et al, OpenAI, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Learning_to_summarize_from_human_feedback_Stiennon_OpenAI_2022.pdf)

* [Mistral AI vs. Meta: Comparing Top Open-source LLMs with Luis Roque, Jan 2024](https://towardsdatascience.com/mistral-ai-vs-meta-comparing-top-open-source-llms-565c1bc1516e)

* [Text Embeddings, Classification, and Semantic Search, Shaw Talebi, March 2024](https://towardsdatascience.com/text-embeddings-classification-and-semantic-search-8291746220be)

* [How to Build a Local Open-Source LLM Chatbot With RAG, Dr. Leon Eversberg, April, 2024](https://towardsdatascience.com/how-to-build-a-local-open-source-llm-chatbot-with-rag-f01f73e2a131)
