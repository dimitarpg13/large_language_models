# Large Language Models Resources

## articles

* [Chain of Code: Reasoning with a Language Model-Augmented Code Emulator, Li et al, DeepMind, Stanford U., UC Berkeley, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Chain_of_Code-Reasoning_with_a_Language_Model-Augmented_Code_Emulator_DeepMind_2023.pdf)

    related repo: https://sites.google.com/view/chain-of-code

* [Llama 2: Open Foundation and Fine-Tuned Chat Models, Hugo Touvron, Louis Martin, et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/LLaMAOpenandEfficientFoundationLanguageModels_MetaAI_2023.pdf)

* [GPT-4 Technical Report, OpenAI, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/GPT-4_Technical_Report_OpenAI_2023.pdf)

* [The Dawn of LMMs: Preliminary Explorations with GPT-4Vision, Yang et al, Microsoft, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/The_Dawn_of_LMMs-Preliminary_Explorations_with_GPT-4Vision_Yang_Microsoft_2023.pdf)

* [Sparks of artificial general intelligence: early experiments with GPT-4, Microsoft Research](https://github.com/dimitarpg13/large_language_models/blob/main/articles/SparksofArtificialGeneralIntelligenceEarlyExperimentswithGPT4.pdf)

* [Large Language Models can learn rules, Zhu et al, DeepMind, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Large_Language_Models_Can_Learn_Rules_Zhu_DeepMind_2023.pdf)

* [LLaMA: Open and Efficient Foundation Language Models, Meta AI](https://github.com/dimitarpg13/large_language_models/blob/main/articles/LLaMAOpenandEfficientFoundationLanguageModels_MetaAI_2023.pdf)

* [Physics of Large Language Models (Part 1), Context-free Grammar, Meta FAIR Labs, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Physics_of_Language_Models-Part%201_Context-Free_Grammar_Meta_FAIR_Labs_2023.pdf)

* [Evaluating Large Language Models is a minefield, A. Narayan, S. Kapoor, Princeton U., 2023, online blog](https://www.cs.princeton.edu/~arvindn/talks/evaluating_llms_minefield/)

* [Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks, M. Mitchell et al, Santa Fe Institute, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Comparing_Humans_GPT-4_and_GPT-4V_On_Abstraction_and_Reasoning_Tasks_Mitchell_SantaFe_2023.pdf)

* [Understanding LLMs: A Comprehensive Overview from Training to Inference, Liu et al, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Understanding_LLMs-A_Comprehensive_Overview_from_Training_to_Inference_Liu_2024.pdf)

* [Multimodality and Large Multimodal Models (LMMs), Chip Huyen, 2023, online article](https://huyenchip.com/2023/10/10/multimodal.html)

* [Branch-Solve-Merge Improves Large Language Model Evaluation and Generation, S. Saha et al, UNC Chapel Hill, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Branch-Solve-Merge_Improves_Large_Language_Model_Evaluation_and_Generation_Saha_UNCChapelHill_2023.pdf)

* [Transformers Learn In-Context by Gradient Descent, Oswald et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Transformers_Learn_In-Context_by_Gradient_Descent_Oswald_2023.pdf)

* [Transformers as Algorithms: Generalization and Stability in In-context Learning, Li et al., 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Transformers_as_Algorithms-Generalization_and_Stability_in_In-context_Learning_Li_2023.pdf)

* [Hyena Hierarchy: Towards Larger Convolutional Language Models, Poli et al., 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Hyena_Hierarchy-Towards_Larger_Convolutional_Language_Models_Poli_Bengio_2023.pdf)

* [Toward Understanding Why Adam Converges faster Than SGD for Transformers, Pan et al., CMU, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Toward_Understanding_Why_Adam_Converges_Faster_Than_SGD_for_Transformers_CMU_2023.pdf)

* [Can GPT-3 Perform Statutory Reasoning?, Blair-Stanek, A et al., 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Can_GPT-3_Perform_Statutory_Reasoning_Blair-Stanek_2023.pdf)

* [Solving olympiad geometry without human demonstrations, TH Trinh et al, DeepMind, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Solving_olympiad_geometry_without_human_demonstrations_Trinh_DeepMind_2023.pdf)

* [An Explanation of In-context Learning as Implicity Bayesian Inference, Xie et al., Stanford, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/An_Explanation_of_In-context_Learning_as_Implicit_Bayesian_Inference_Stanford_2022.pdf)

* [Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions, S. Bhattamishra, Oxford U., 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Understanding_In-Context_Learning_in_Transformers_and_LLMs_by_Learning_to_Learn_Discrete_Functions_Bhattamishra_2023.pdf)

* [Efficient Transformers: A Survey, Tay et al., Google Research, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Efficient_Transformers-A_Survey_GoogleResearch_2022.pdf)

* [Emergent Abilities of Large Language Models, Wei et al., Google Research, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Emergent_Abilities_of_Large_Language_Models_GoogleResearch_2022.pdf)

* [A Path Towards Autonomous Machine Intelligence, Yann LeCun, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/A_Path_Towards_Autonomous_Machine_Intelligence_LeCunn_2022.pdf)

* [Holisitc Evaluation of Language Models, Center for Research on Foundation Models, Stanford, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Holistic_Evaluation_of_Language_Models_CRFM_StanfordU_2022.pdf)

* [A Systematic Evaluation of Large Language Models of Code, Xu et al., CMU, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/A_Systematic_Evaluation_of_Large_Language_Models_of_Code_CMU_2022.pdf)

* [Evaluating Large Language Models Trained on Code, Chen et al., OpenAI, 2021](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Evaluating_Large_Language_Models_Trained_on_Code_OpenAI_2021.pdf)

* [Language Models are Few-Shot Learners, Brown et al., OpenAI, 2020](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Language_Models_are_Few-ShotLearners_OpenAI2020.pdf)

* [Program Synthesis, Gulwani et al., Microsoft Research, 2017](https://github.com/dimitarpg13/large_language_models/blob/main/articles/program_synthesis_Gulwani_MicrosfotResearch_2017.pdf)

* [Adam: A Method for Stochasitc Optimization, D. Kingma et al, 2014](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Adam-A_method_for_stochastic_optimization_Kingma_2014.pdf)

* [Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning, Roemmele et al, 2011](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Choice_of_Plausible_Alternatives-An_Evaluation_of_Commonsense_Causal_Reasoning_Roemmele_2011.pdf)

* [Catastrophic Interference In Connectionist Networks: The Sequential Learning Problem, McCloskey, Cohen, 1989](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Catastrophic_Interference_in_Connectionist_Networks-The_Sequential_Learning_Problem_Mccloskey_Cohen_1989.pdf)

* [Attention Is All You Need, Vaswani et al, Google Brain, 2017](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Attention-is-all-you-need-NIPS-2017.pdf)

* [HyperAttention: Long-context Attention in Near-Linear Time, Insu Han et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/HyperAttention-Long-context_Attention_in_Near-Linear_Time_Han_2023.pdf)

* [The Annotated Transformer, 2018](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TheAnnotatedTransformer.pdf)

* [The Illustrated Transformer, Jay Alamar's blog, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TheIllustratedTransformer%E2%80%93JayAlammar%E2%80%93Visualizing_machine_learning_one_concept_at_a_time.pdf)

* [Attention in Natural Language Processing, Galassi et al., 2020](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/AttentionInNaturalLanguageProcessing.pdf)

* [Vision Language Transformers: A Survey, Clayton Fields, Casey Kennington, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Vision_Language_Transformers-A_Survey_Fields_2023.pdf)

* [Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization, Jin et al, Peking U., 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Unified_Language-Vision_Pretraining_in_LLM_with_Dynamic_Discrete_Visual_Tokenization_Jin_2023.pdf)

* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al., Google AI, 2019](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/BERTPretrainingofDeepBidirectionalTransformersforLanguageUnderstanding.pdf)

* [FAIRSEQ: A Fast, Extensible Toolkit for Sequence Modeling, Ott et al., 2019](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/FairseqAFastExtensibleToolkitForSequenceModeling.pdf)

* [Autoencoders, Dor Bank et al, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Autoencoders.pdf)

* [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling, Chung et al., 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/EmpiricalEvaluationOfGatedRecurrentNeuralNetworksonSequenceModeling.pdf)

* [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches, Cho et al., U de Montreal, 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/OnthePropertiesOfNeuralMachineTranslationEncoderDecoderApproaches.pdf)

* [Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network, A. Sherstinsky, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/FundamentalsOfRNNandLSTMNetwork.pdf)

* [A Decomposable Attention Model for Natural Language Inference, Parikh et al., Google Research, 2016](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/DecomposableAttentionModelforNaturalLanguageInferenceParikhUszkoreit2016.pdf)

* [Sequence to Sequence Learning with Neural Networks, Sutskever et al, Google Research, 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/SequencetoSequenceLearningwithNeuralNetworksSutsekver2014.pdf)

* [Transforming Auto-encoders, G. Hinton, A. Krizhevsky, et al., 2011](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TransformingAutoencodersHinton.pdf)

* [Long Short-Term Memory, Sepp Hochreiter et al., 1997](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/LongShortTermMemory.pdf)

* [Understanding LSTM: a tutorial into Long Short-Term Memory, R. Staudemeyer et al., 2019](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TutorialOnLongShortTermMemory2019.pdf)

* [What Can Transformers Learn in Context? A Case Study of Simple Function Classes, Carg S., et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/What_Can_Transformers_Learn%20In-Context-A_Case_Study_of_Simple_Function_Classes_Stanford_2023.pdf)

* [Toward Understanding Why Adam Converges Faster Than SGD for Transformers, Pan, Y, et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Toward_Understanding_Why_Adam_Converges_Faster_Than_SGD_for_Transformers_CMU_2023.pdf)

* [How ChatGPT Behavior is Changing Over Time?, Chen, L, Stanford, UC Berkeley, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/How_ChatGPT_behavior_is_changing_over_time_2023.pdf)

* [What Is ChatGPT Doing and Why Does It Work?, S. Wolfram, Feb 2023, online article](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)

* [Retentive Network: A Successor to Transformer for Large Language Models, Sun, Y., Microsoft Research, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Retentive_Network-A_Successor_to_Transformer_for_Large_Language_Models_2023.pdf)

* [Meta-Transformer: A Unifed Framework for Multi-Modal Learning, Zhang, Y., et al, 2023](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Meta-Transformer_Unified_Framework_for_Multi-Modal_Learning_2023.pdf)

* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin, Google AI, 2019](https://github.com/dimitarpg13/large_language_models/blob/main/articles/BERT-Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding_Devlin_Google_2019.pdf)

* [Improving Language Understanding by Generative Pre-Training, A. Redford, Open AI, 2018](https://github.com/dimitarpg13/large_language_models/tree/main/articles/Improving_Language_Understanding_by_Generative_Pre-Training_Redford_OpenAI_2018.pdf)

* [Llama 2: Open Foundation and Fine-Tuned Chat Models, H. Touvron, Meta, 2023](https://github.com/dimitarpg13/large_language_models/tree/main/articles/Llama2-Open_Foundation_and_Fine-Tuned_Chat_Models_Touvron_Meta_2023.pdf)

* [Scaling Language Models - Methods, Analysis and Insights from Training Gopher,JW Rae, DeepMind, 2021](https://github.com/dimitarpg13/large_language_models/tree/main/articles/Scaling_Language_Models-Methods_Analysis_Insights_from_Training_Gopher_Rae_DeepMind_2021.pdf)

* [... More articles on Transformers](https://github.com/dimitarpg13/transformers_intro/tree/main/articles_and_books)

* [...More LLM articles on this repo](https://github.com/dimitarpg13/large_language_models/tree/main/articles)

## online videos

* [How ChatGPT is Trained with Ari Seff (February, 2023)](https://www.youtube.com/watch?v=VPRSBzXzavo)

* [Let's build GPT: from scratch, in code, spelled out with Andrej Karpathy (February 2023)](https://www.youtube.com/watch?v=kCc8FmEb1nY)

## LLM visualization

The resource below attempts to visualize what is happening in LLM under the hood and is a helpful tool to comprehend the work of decoder-only Transformer-based LLMs. The author Brendan Bycroft has made an interesting attempt to visualize these structures and clarify how they operate. This webpage in the link below provides visualization for a family of GPT models, presented in 3D animations with walkthrough. The tool provides a step-by-step guide for single-token inference, coupled with interactive elements for a hands-on experience.

https://bbycroft.net/llm


## medium

* [The Transformer Architecture of GPT Models with Beatriz Stollniz](https://towardsdatascience.com/the-transformer-architecture-of-gpt-models-b8695b48728b)

* [Learning Transformers Code First Part 1 - The Setup with Lily Hughs-Robinson](https://towardsdatascience.com/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0)

* [Learning Transformers Code First Part 2 - GPT Up Close and Personal with Lily Hughs-Robinson](https://towardsdatascience.com/learning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7)

* [Understanding Large Language Models: The Physics of ChatGPT and BERT with Tim Lou](https://towardsdatascience.com/understanding-large-language-models-the-physics-of-chat-gpt-and-bert-ea512bcc6a64)

* [Transformer Architectures and the Rise of BERT, GPT, and T5: A Beginner's Guide with Manas Joshi](https://pub.towardsai.net/transformer-architectures-and-the-rise-of-bert-gpt-and-t5-a-beginners-guide-ea5e5bca923c)

* [Inside GPT - I: Understanding the text generation with Fatih Demirci](https://towardsdatascience.com/inside-gpt-i-1e8840ca8093)

* [Platypus: Quick, Cheap and Powerful LLM with Salvatore Raieli](https://levelup.gitconnected.com/platypus-quick-cheap-and-powerful-llm-404b86af8755)

* [Configuring Nemo-Guardrails Your Way: An Alternative Method for LLM with Masatake Hirono](https://towardsdatascience.com/configuring-nemo-guardrails-your-way-an-alternative-method-for-large-language-models-c82aaff78f6e)

* [ChatGPT stories compiled by Mateusz Wasalski](https://medium.com/@m.wasalski/list/chatgpt-3742c7a4727d)

* [RetNet: Transformer killer is here with Vishal Rajput](https://medium.com/aiguys/retnet-transformer-killer-is-here-1dc7f50d1205)

* [Fine-Tuning Large Language Models (LLMs) with Shawhin Talebi](https://towardsdatascience.com/fine-tuning-large-language-models-llms-23473d763b91)

* [How to Build an LLM from Scratch with Shawhin Talebi](https://towardsdatascience.com/how-to-build-an-llm-from-scratch-8c477768f1f9)

* [Conversations as Directed Graphs with LangChain with Daniel Warfield](https://towardsdatascience.com/conversations-as-directed-graphs-with-lang-chain-46d70e1a846c)

* [Mastering Language Models with Samuel Montgomery](https://towardsdatascience.com/mastering-language-models-32e1d891511a)

* [Self-Supervised Learning Using Projection Heads with Daniel Warfield](https://towardsdatascience.com/self-supervised-learning-using-projection-heads-b77af3911d33)

* [Summing Coin Values in Images using Lang-SAM and Deep Learning with Gamze Zorlubas](https://towardsdatascience.com/coin-counting-using-lang-sam-b469827808a7)

* [‘Talk’ to Your SQL Database Using LangChain and Azure OpenAI with Satwiki De](https://towardsdatascience.com/talk-to-your-sql-database-using-langchain-and-azure-openai-bb79ad22c5e2)

* [RLHF: Reinforcement Learning from Human Feedback with Ms Aerin](https://automata88.medium.com/rlhf-reinforcement-learning-from-human-feedback-faa5ff4761d1)

    related paper: [Training language models to follow instructions
with human feedback, Ouyang et al, OpenAI, 2022](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Training_language_models_to_follow_instructions_with_human_feedback_Ouyang_2022.pdf)

    related code: [Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture](https://github.com/lucidrains/PaLM-rlhf-pytorch/tree/main)


* [How to Convert Any Text Into a Graph of Concepts with Rahul Nayak](https://towardsdatascience.com/how-to-convert-any-text-into-a-graph-of-concepts-110844f22a1a)

    related repo: [Knowledge_Graph](https://github.com/rahulnyk/knowledge_graph)

* [LLMs for Everyone: Running LangChain and a MistralAI 7B Model in Google Colab with Dmitrii Eliuseev](https://towardsdatascience.com/llms-for-everyone-running-langchain-and-a-mistralai-7b-model-in-google-colab-246ca94d7c4d)

* [LLMs for Everyone: Running the LLaMA-13B model and LangChain in Google Colab with Dmitrii Eliuseev](https://towardsdatascience.com/llms-for-everyone-running-the-llama-13b-model-and-langchain-in-google-colab-68d88021cf0b)

    related repo: https://github.com/ggerganov/llama.cpp

    related repo: https://github.com/langchain-ai/langchain

    related repo: https://colab.research.google.com/

* [Is Mamba the End of ChatGPT As We Know It? Igancio de Gregorio](https://pub.towardsai.net/is-mamba-the-end-of-chatgpt-as-we-know-it-a2ce57de0b02)

    related paper: [Mamba: Linear-Time Sequence Modeling with Selective State Spaces, A. Gu et al, CMU, 2024](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Mamba-Linear-Time_Sequence_Modeling_with_Selective_State_Spaces_Gu_CMU_2024.pdf)

