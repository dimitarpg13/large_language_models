# Learn Large Language Models by Doing

[Can you really run LLMs locally on your laptop?, James Reagan, Dec 26, 2023, online blog](https://jpreagan.com/blog/can-you-really-run-llms-locally-on-your-laptop)

[The spelled-out intro to neural networks and backpropagation: building micrograd, Andrej Karpathy, Feb, 2023, youtube video](https://youtu.be/VMj-3S1tku0)

[The spelled-out intro to language modeling: building makemore, Andrej Karpathy, Feb, 2023, youtube video](https://youtu.be/PaCmpygFfXo)

[Building makemore Part 2: MLP, Andrej Karpathy, Feb, 2023, youtube video](https://youtu.be/PaCmpygFfXo)

[Building makemore Part 3: Activations & Gradients, BatchNorm, Andrej Karpathy, Feb, 2023, youtube video](https://youtu.be/P6sfmUTpUmc)

[Building makemore Part 4: Becoming a Backprop Ninja, Andrej Karpathy, Feb, 2023, youtube video](https://youtu.be/q8SA3rM6ckI)

[Building makemore Part 5: Building a WaveNet, Andrej Karpathy, Feb, 2023, youtube video](https://youtu.be/t3YJ5hKiMQ0)

[Let's build GPT: from scratch, in code, spelled out, Andrej Karpathy, Feb, 2023, youtube video](https://youtu.be/kCc8FmEb1nY)

[NLP From Scratch: Translation with a Sequence to Sequence Network and Attention, Sean Robertson, PyTorch Tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)

   related Github repo: https://github.com/pytorch/tutorials/blob/main/intermediate_source/seq2seq_translation_tutorial.py

[The Minimal Byte-Pair Encoding Algorithm MinBPE for Language Tokenization, Andrej Karpathy's Github repo](https://github.com/karpathy/minbpe/tree/master)

[GPT in 60 Lines of NumPy, Jay Mody, Jan 30, 2023, online blog](https://jaykmody.com/blog/gpt-from-scratch/)

[No GPU? No problem. localllm lets you develop gen AI apps on local CPUs, Geoffrey Anderson, Christie Warwick, Google Cloud](https://cloud.google.com/blog/products/application-development/new-localllm-lets-you-develop-gen-ai-apps-locally-without-gpus)

[How to Build a Local Open-Source LLM Chatbot With RAG: Talking to PDF documents with Google’s Gemma-2b-it, LangChain, and Streamlit, Dr. Leon Eversberg, April 2024, TDS Medium](https://towardsdatascience.com/how-to-build-a-local-open-source-llm-chatbot-with-rag-f01f73e2a131)

[A BERT for laptops, from scratch, github repo](https://github.com/samvher/bert-for-laptops/blob/main/BERT_for_laptops.ipynb)

[The easiest way to run an LLM locally on your Mac, Jeremy Morgan, Jan 2024, online blog](https://www.jeremymorgan.com/blog/generative-ai/how-to-llm-local-mac-m1/)

[Can You Run a Large Language Model (LLM) Locally on an M1 MacBook Air with only 16 GB of Memory ? Dennis Layton, Jan 2024, Medium](https://medium.com/@dlaytonj2/can-you-run-a-large-language-model-llm-locally-on-an-m1-macbook-air-with-only-16-gb-of-memory-cd9741af27bb)

[Run Llama Without a GPU! Quantized LLM with LLMWare and Quantized Dragon, Shanglun Wang, Jan 7th, 2024](https://hackernoon.com/run-llama-without-a-gpu-quantized-llm-with-llmware-and-quantized-dragon)

[How to Locally Run a ChatGPT-Like LLM on Your PC and Mac, Arjun Sha, Sept 2023, online blog](https://beebom.com/how-run-chatgpt-like-language-model-pc-offline/)

[Running LLM’s Locally: A Step-by-Step Guide, mydeveloperplanet, Dec 2023, online blog](https://mydeveloperplanet.com/2023/12/13/running-llms-locally-a-step-by-step-guide/)


## a bit of Theory + jupyter notebooks on Transformers

[Natural Language Processing with Transformers, Lewis Tunstall, Leandro von Werra, Thomas Wolf, 2022](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/natural-language-processing-with-transformers-revised-edition-book.pdf)

   github repo with source code: https://github.com/nlp-with-transformers/notebooks

[Learning Transformers Code First: Part 1 — The Setup, Lily Hughes-Robinson, July, 2023](https://towardsdatascience.com/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0)

[Learning Transformers Code First Part 2 — GPT Up Close and Personal, Lily Hughes-Robinson, July, 2023](https://towardsdatascience.com/learning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7)


[The A-Z of Transformers: Everything You Need to Know with François Porcher](https://towardsdatascience.com/the-a-z-of-transformers-everything-you-need-to-know-c9f214c619ac)
    
related paper: [Neural Machine Translation by Jointly Learning to Align and Translate, D. Bahdanau et al, 2015](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/NeuralMachineTranslationByJointlyLearningToAlignAndTranslateBahdanau2015.pdf)

related repo: [Transformers from Scratch](https://github.com/FrancoisPorcher/awesome-ai-tutorials/tree/main/NLP/007%20-%20Transformers%20From%20Scratch)

[Transformers — Intuitively and Exhaustively Explained with Daniel Warfield](https://towardsdatascience.com/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)

[De-coded: Transformers explained in plain English with Chris Hughes](https://towardsdatascience.com/de-coded-transformers-explained-in-plain-english-877814ba6429)

[Transformers Explained Visually — Not Just How, but Why They Work So Well, Ketan Doshi, Jun 2, 2021](https://towardsdatascience.com/transformers-explained-visually-not-just-how-but-why-they-work-so-well-d840bd61a9d3)

[Transformers Explained Visually (Part 1): Overview of Functionality with Ketan DOshi, Dec 13, 2020](https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452)

[Transformers Explained Visually (Part 2): How it works, step-by-step with Ketan Doshi, Jan 2, 2021](https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34)

[Transformers Explained Visually (Part 3): Multi-head Attention, deep dive, Jan 16, 2020](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)

# Frameworks and software packages

## Starcoder

https://techcrunch.com/2024/02/28/starcoder-2-is-a-code-generating-ai-that-runs-on-most-gpus/

https://developer.nvidia.com/blog/unlock-your-llm-coding-potential-with-starcoder2/

https://huggingface.co/blog/starcoder2

[StarCoder 2 and The Stack v2: The Next Generation, Anthon Lozhkov et al, NVIDIA, various universitites, 2024](https://drive.google.com/file/d/17iGn3c-sYNiLyRSY-A85QOzgzGnGiVI3/view)

https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/starcoder2-15b

https://huggingface.co/bigcode/starcoder2-15b

## Ray

[Modern Parallel and Distributed Python: A Quick Tutorial on Ray, Robert Nishihara, Feb 2019](https://towardsdatascience.com/modern-parallel-and-distributed-python-a-quick-tutorial-on-ray-99f8d70369b8)
